# ğŸ—ï¸ Data Lakehouse - Open Source Platform

> **DATAFLOW 2026: THE ALCHEMY OF MINDS**
> 
> Full-stack Open-Source Data Lakehouse Platform sá»­ dá»¥ng kiáº¿n trÃºc Medallion

[![Docker](https://img.shields.io/badge/Docker-Ready-blue?logo=docker)](https://docker.com)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange?logo=apachespark)](https://spark.apache.org)
[![Apache Iceberg](https://img.shields.io/badge/Apache%20Iceberg-1.4-blue)](https://iceberg.apache.org)
[![ClickHouse](https://img.shields.io/badge/ClickHouse-24.1-yellow?logo=clickhouse)](https://clickhouse.com)
[![Apache Superset](https://img.shields.io/badge/Apache%20Superset-3.1-cyan)](https://superset.apache.org)

---

## ğŸ“‹ Má»¥c Lá»¥c

- [Giá»›i Thiá»‡u](#-giá»›i-thiá»‡u)
- [Kiáº¿n TrÃºc Há»‡ Thá»‘ng](#-kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [CÃ´ng Nghá»‡ Sá»­ Dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [YÃªu Cáº§u Há»‡ Thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Äáº·t](#-cÃ i-Ä‘áº·t)
- [HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [Data Pipeline](#-data-pipeline)
- [Schema Evolution Demo](#-schema-evolution-demo)
- [Dashboards](#-dashboards)
- [Cáº¥u TrÃºc Dá»± Ãn](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ“– TÃ i Liá»‡u HÆ°á»›ng Dáº«n

> **ğŸ¯ Báº¯t Ä‘áº§u nhanh?** Xem [QUICK_START.md](./QUICK_START.md) - HÆ°á»›ng dáº«n cháº¡y project trong 5 phÃºt

> **ğŸ“˜ Cáº§n hÆ°á»›ng dáº«n chi tiáº¿t?** Xem [HUONG_DAN_CHAY_PROJECT.md](./HUONG_DAN_CHAY_PROJECT.md) - TÃ i liá»‡u Ä‘áº§y Ä‘á»§ tá»« setup Ä‘áº¿n troubleshooting

> **ğŸ“ Muá»‘n hiá»ƒu cáº¥u trÃºc project?** Xem [CAU_TRUC_PROJECT.md](./CAU_TRUC_PROJECT.md) - Giáº£i thÃ­ch chi tiáº¿t tá»«ng thÆ° má»¥c vÃ  file

---

## ğŸ¯ Giá»›i Thiá»‡u

### BÃ i ToÃ¡n

Dá»± Ã¡n xÃ¢y dá»±ng há»‡ thá»‘ng **Data Lakehouse** hoÃ n chá»‰nh tá»« con sá»‘ 0, sá»­ dá»¥ng cÃ¡c cÃ´ng nghá»‡ mÃ£ nguá»“n má»Ÿ Ä‘á»ƒ thay tháº¿ cÃ¡c dá»‹ch vá»¥ cloud managed:

| Cloud Service | Open-Source Alternative |
|--------------|------------------------|
| AWS S3 | **MinIO** |
| Databricks | **Apache Spark + Iceberg** |
| Snowflake | **ClickHouse** |

### Use Case: E-commerce Event History

**Dataset**: [eCommerce Events History in Cosmetics Shop](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop)

- **Quy mÃ´**: ~20 triá»‡u events
- **Loáº¡i dá»¯ liá»‡u**: User behavior (view, cart, purchase)
- **Thá»i gian**: Oct 2019 - Apr 2020

### BÃ i ToÃ¡n Nghiá»‡p Vá»¥

1. ğŸ“Š **PhÃ¢n tÃ­ch phá»…u chuyá»ƒn Ä‘á»•i** (Funnel Analysis)
2. ğŸ’° **PhÃ¢n tÃ­ch doanh thu theo thá»i gian** (Revenue Analysis)
3. ğŸ‘¥ **PhÃ¢n khÃºc khÃ¡ch hÃ ng RFM** (Customer Segmentation)

---

## ğŸ›ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

### Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VISUALIZATION LAYER                          â”‚
â”‚                         Apache Superset                             â”‚
â”‚                    (Dashboard & BI Reports)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SERVING LAYER                                â”‚
â”‚                          ClickHouse                                  â”‚
â”‚               (OLAP Database - Sub-second Queries)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRANSFORMATION LAYER                            â”‚
â”‚              dbt (data build tool) + Apache Spark                    â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚    BRONZE    â”‚â”€â”€â”€â–¶â”‚    SILVER    â”‚â”€â”€â”€â–¶â”‚     GOLD     â”‚           â”‚
â”‚  â”‚  (Raw Data)  â”‚    â”‚(Cleaned Data)â”‚    â”‚(Aggregated)  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TABLE FORMAT                                  â”‚
â”‚                       Apache Iceberg                                 â”‚
â”‚    [Schema Evolution, Time Travel, Partitioning, Z-Ordering]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STORAGE LAYER                                 â”‚
â”‚                           MinIO                                      â”‚
â”‚                (S3-Compatible Object Storage)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
CSV Files â†’ Spark â†’ Bronze (Iceberg) â†’ Silver (Iceberg) â†’ Gold (Iceberg) â†’ ClickHouse â†’ Superset
```

---

## ğŸ”§ CÃ´ng Nghá»‡ Sá»­ Dá»¥ng

| Táº§ng | CÃ´ng Nghá»‡ | PhiÃªn Báº£n | Má»¥c ÄÃ­ch |
|------|-----------|-----------|----------|
| Storage | MinIO | 2024.01 | Object Storage (S3-compatible) |
| Table Format | Apache Iceberg | 1.4.3 | ACID transactions, Schema Evolution |
| Compute | Apache Spark | 3.5.0 | Distributed Processing |
| Transformation | dbt | 1.7+ | Data Modeling |
| Serving | ClickHouse | 24.1 | OLAP Queries |
| Visualization | Apache Superset | 3.1.0 | Dashboards & BI |
| Orchestration | Docker Compose | 3.8 | Container Management |

---

## ğŸ’» YÃªu Cáº§u Há»‡ Thá»‘ng

### Hardware

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| RAM | 8 GB (Ä‘Ã£ tá»‘i Æ°u vá»›i cáº¥u hÃ¬nh lightweight) | **16 GB** (thoáº£i mÃ¡i hÆ¡n, nháº¥t lÃ  khi cháº¡y full pipeline + ClickHouse) |
| CPU | 4 cores | 8 cores |
| Disk | 20 GB | 50 GB |

### Software

- Docker Desktop 4.0+
- Docker Compose 2.0+
- Make (optional, cho automation)
- Python 3.9+ (cho Superset setup)

---

## ğŸš€ CÃ i Äáº·t & CÃ¡ch Cháº¡y Project

> **âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG:** Project nÃ y **KHÃ”NG sá»­ dá»¥ng AWS services**. Táº¥t cáº£ Ä‘á»u dÃ¹ng open-source self-hosted:
> - **MinIO** thay S3
> - **Apache Spark + Iceberg** thay Databricks  
> - **ClickHouse** thay Snowflake
> - Sá»­ dá»¥ng **HadoopFileIO** thay vÃ¬ AWS S3FileIO

### BÆ°á»›c 1: Clone Repository

```bash
git clone https://github.com/thequang05/auto_scaling.git
cd auto_scaling
```

### BÆ°á»›c 2: Táº£i Dataset

Download dataset tá»« Kaggle vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `data/raw/`:

```bash
# Táº¡o thÆ° má»¥c
mkdir -p data/raw

# Download tá»« Kaggle:
# https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop

# Giáº£i nÃ©n vÃ  Ä‘áº·t cÃ¡c file CSV vÃ o data/raw/
# VÃ­ dá»¥: 2019-Oct.csv, 2019-Nov.csv, ...
```

### BÆ°á»›c 3: Build vÃ  Khá»Ÿi Ä‘á»™ng Services

```powershell
# Di chuyá»ƒn vÃ o thÆ° má»¥c docker
cd docker

# Build images
docker compose build

# Khá»Ÿi Ä‘á»™ng Táº¤T Cáº¢ services
docker compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker ps
```

**Äá»£i khoáº£ng 1-2 phÃºt Ä‘á»ƒ táº¥t cáº£ services healthy.**

**Services sau khi khá»Ÿi Ä‘á»™ng:**

| Service | URL | Credentials |
|---------|-----|-------------|
| MinIO Console | http://localhost:9001 | minioadmin / minioadmin123 |
| Iceberg REST | http://localhost:8181 | - |
| Spark Master UI | http://localhost:8080 | - |
| ClickHouse | http://localhost:8123 | default / clickhouse123 |
| Superset | http://localhost:8088 | admin / admin |

---

## ğŸ“– HÆ°á»›ng Dáº«n Cháº¡y Data Pipeline

### BÆ°á»›c 1: Bronze Layer - Ingestion

Ingest dá»¯ liá»‡u CSV vÃ o Bronze layer (Iceberg tables trÃªn MinIO):

```powershell
docker exec spark-master spark-submit `
    --master spark://spark-master:7077 `
    --conf spark.driver.memory=2g `
    --conf spark.executor.memory=2g `
    /opt/spark-apps/jobs/bronze/ingest_events.py
```

**Káº¿t quáº£:** ~20 triá»‡u records Ä‘Æ°á»£c ghi vÃ o `iceberg.bronze.events_raw`

### BÆ°á»›c 2: Silver Layer - Transformation

LÃ m sáº¡ch vÃ  transform dá»¯ liá»‡u:

```powershell
docker exec spark-master spark-submit `
    --master spark://spark-master:7077 `
    --conf spark.driver.memory=2g `
    --conf spark.executor.memory=2g `
    /opt/spark-apps/jobs/silver/clean_events.py
```

**Káº¿t quáº£:** Dá»¯ liá»‡u Ä‘Æ°á»£c deduplicate, xá»­ lÃ½ NULL, vÃ  chuáº©n hÃ³a

### BÆ°á»›c 3: Gold Layer - Aggregation

Táº¡o cÃ¡c báº£ng aggregate cho business analytics:

```powershell
docker exec spark-master spark-submit `
    --master spark://spark-master:7077 `
    --conf spark.driver.memory=2g `
    --conf spark.executor.memory=2g `
    /opt/spark-apps/jobs/gold/aggregate_sales.py
```

**Káº¿t quáº£:** 4 báº£ng Gold Ä‘Æ°á»£c táº¡o:
- `iceberg.gold.daily_sales` - Doanh thu theo ngÃ y/category
- `iceberg.gold.funnel_analysis` - PhÃ¢n tÃ­ch funnel chuyá»ƒn Ä‘á»•i
- `iceberg.gold.customer_rfm` - PhÃ¢n khÃºc khÃ¡ch hÃ ng RFM
- `iceberg.gold.product_performance` - Hiá»‡u suáº¥t sáº£n pháº©m

### BÆ°á»›c 4: Sync Gold Layer â†’ ClickHouse

Export dá»¯ liá»‡u tá»« Iceberg sang ClickHouse cho serving layer:

**4.1. Táº¡o báº£ng trong ClickHouse:**

```powershell
docker exec clickhouse clickhouse-client --password clickhouse123 --multiquery --query "
CREATE DATABASE IF NOT EXISTS lakehouse;

CREATE TABLE IF NOT EXISTS lakehouse.daily_sales (
    event_date Date,
    category_level1 String,
    category_level2 String,
    order_count UInt64,
    unique_customers UInt64,
    unique_products UInt64,
    total_revenue Float64,
    avg_order_value Float64,
    min_order_value Float64,
    max_order_value Float64,
    revenue_per_customer Float64,
    sale_year UInt16,
    sale_month UInt8,
    sale_quarter UInt8,
    sale_week UInt8,
    _aggregated_at DateTime64(6)
) ENGINE = MergeTree() ORDER BY (event_date, category_level1);

CREATE TABLE IF NOT EXISTS lakehouse.funnel_analysis (
    event_date Date,
    category_level1 String,
    views UInt64,
    carts UInt64,
    purchases UInt64,
    unique_viewers UInt64,
    unique_carters UInt64,
    unique_purchasers UInt64,
    total_revenue Float64,
    view_to_cart_rate Float64,
    cart_to_purchase_rate Float64,
    overall_conversion_rate Float64,
    user_view_to_cart_rate Float64,
    user_cart_to_purchase_rate Float64,
    avg_revenue_per_purchaser Float64,
    analysis_year UInt16,
    analysis_month UInt8,
    _aggregated_at DateTime64(6)
) ENGINE = MergeTree() ORDER BY (event_date, category_level1);

CREATE TABLE IF NOT EXISTS lakehouse.customer_rfm (
    user_id UInt64,
    recency Int32,
    frequency Int64,
    monetary Float64,
    first_purchase_date Date,
    last_purchase_date Date,
    avg_order_value Float64,
    unique_products_bought Int64,
    r_score UInt8,
    f_score UInt8,
    m_score UInt8,
    rfm_score UInt16,
    rfm_string String,
    customer_segment String,
    segment_date Date,
    _aggregated_at DateTime64(6)
) ENGINE = MergeTree() ORDER BY (user_id);

CREATE TABLE IF NOT EXISTS lakehouse.product_performance (
    product_id UInt64,
    category_level1 String,
    category_level2 String,
    brand String,
    view_count UInt64,
    cart_count UInt64,
    purchase_count UInt64,
    unique_viewers UInt64,
    unique_carters UInt64,
    unique_purchasers UInt64,
    total_revenue Float64,
    avg_price Float64,
    min_price Float64,
    max_price Float64,
    view_to_cart_rate Float64,
    cart_to_purchase_rate Float64,
    overall_conversion_rate Float64,
    revenue_per_view Float64,
    _aggregated_at DateTime64(6)
) ENGINE = MergeTree() ORDER BY (product_id);
"
```

**4.2. Export vÃ  Import dá»¯ liá»‡u:**

```powershell
# Export CSV tá»« Iceberg
docker exec spark-master spark-submit `
    --master spark://spark-master:7077 `
    /opt/spark-apps/jobs/serving/export_csv.py

# Import vÃ o ClickHouse
docker exec -i clickhouse bash -c "cat /tmp/daily_sales.csv | clickhouse-client --password clickhouse123 --query 'INSERT INTO lakehouse.daily_sales FORMAT CSVWithNames'"
docker exec -i clickhouse bash -c "cat /tmp/funnel_analysis.csv | clickhouse-client --password clickhouse123 --query 'INSERT INTO lakehouse.funnel_analysis FORMAT CSVWithNames'"
docker exec -i clickhouse bash -c "cat /tmp/customer_rfm.csv | clickhouse-client --password clickhouse123 --query 'INSERT INTO lakehouse.customer_rfm FORMAT CSVWithNames'"
docker exec -i clickhouse bash -c "cat /tmp/product_performance.csv | clickhouse-client --password clickhouse123 --query 'INSERT INTO lakehouse.product_performance FORMAT CSVWithNames'"
```

**4.3. Kiá»ƒm tra káº¿t quáº£:**

```powershell
docker exec clickhouse clickhouse-client --password clickhouse123 --multiquery --query "
SELECT 'daily_sales' as tbl, count() as rows FROM lakehouse.daily_sales;
SELECT 'funnel_analysis' as tbl, count() as rows FROM lakehouse.funnel_analysis;
SELECT 'customer_rfm' as tbl, count() as rows FROM lakehouse.customer_rfm;
SELECT 'product_performance' as tbl, count() as rows FROM lakehouse.product_performance;
"
```

---

## ğŸ“Š BÆ°á»›c 5: Táº¡o Dashboard trong Superset

### 5.1. Truy cáº­p Superset

Má»Ÿ trÃ¬nh duyá»‡t: **http://localhost:8088**
- Username: `admin`
- Password: `admin`

### 5.2. Káº¿t ná»‘i ClickHouse Database

1. VÃ o **Settings** â†’ **Database Connections** â†’ **+ Database**
2. Chá»n **ClickHouse Connect**
3. SQLAlchemy URI:
   ```
   clickhousedb://default:clickhouse123@clickhouse:8123/lakehouse
   ```
4. Click **Test Connection** â†’ **Connect**

### 5.3. Táº¡o Datasets

1. VÃ o **Data** â†’ **Datasets** â†’ **+ Dataset**
2. Táº¡o 4 datasets cho 4 báº£ng:
   - `lakehouse.daily_sales`
   - `lakehouse.funnel_analysis`
   - `lakehouse.customer_rfm`
   - `lakehouse.product_performance`

### 5.4. Táº¡o Charts (4 KPI Charts)

| Chart | Dataset | Type | Cáº¥u hÃ¬nh |
|-------|---------|------|----------|
| Revenue Trend | daily_sales | Line Chart | X: event_date, Y: SUM(total_revenue) |
| Conversion Funnel | funnel_analysis | Bar Chart | Metrics: SUM(views), SUM(carts), SUM(purchases) |
| Customer Segments | customer_rfm | Pie Chart | Dimension: customer_segment, Metric: COUNT(*) |
| Top Categories | product_performance | Bar Chart | X: category_level1, Y: SUM(total_revenue) |

### 5.5. Táº¡o Dashboard

1. **Dashboards** â†’ **+ Dashboard**
2. Äáº·t tÃªn: `E-commerce Analytics`
3. KÃ©o tháº£ 4 charts vÃ o dashboard
4. **Save**

---

## ğŸ› ï¸ CÃ¡c Lá»‡nh ThÆ°á»ng DÃ¹ng

### Docker Commands (Windows PowerShell)

```powershell
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
cd docker
docker compose up -d

# Dá»«ng táº¥t cáº£ services
docker compose down

# Xem logs
docker compose logs -f

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker ps

# Restart má»™t service cá»¥ thá»ƒ
docker compose restart clickhouse
```

### Spark Commands

```powershell
# Cháº¡y Bronze layer
docker exec spark-master spark-submit --master spark://spark-master:7077 /opt/spark-apps/jobs/bronze/ingest_events.py

# Cháº¡y Silver layer
docker exec spark-master spark-submit --master spark://spark-master:7077 /opt/spark-apps/jobs/silver/clean_events.py

# Cháº¡y Gold layer
docker exec spark-master spark-submit --master spark://spark-master:7077 /opt/spark-apps/jobs/gold/aggregate_sales.py

# Má»Ÿ PySpark Shell
docker exec -it spark-master pyspark --master spark://spark-master:7077
```

### ClickHouse Commands

```powershell
# Má»Ÿ ClickHouse client
docker exec -it clickhouse clickhouse-client --password clickhouse123

# Query trá»±c tiáº¿p
docker exec clickhouse clickhouse-client --password clickhouse123 --query "SELECT count() FROM lakehouse.daily_sales"

# Xem táº¥t cáº£ tables
docker exec clickhouse clickhouse-client --password clickhouse123 --query "SHOW TABLES FROM lakehouse"
```

---

## ğŸ”„ Data Pipeline

### Bronze Layer (Raw Data)

```python
# spark/jobs/bronze/ingest_events.py

# Äá»c CSV â†’ Ghi Iceberg vá»›i metadata
bronze_df = spark.read.csv("data/raw/*.csv")
bronze_df.withColumn("_ingestion_time", current_timestamp())
         .withColumn("_source_file", input_file_name())
         .writeTo("iceberg.bronze.events_raw")
         .partitionedBy("event_date")
         .create()
```

**Báº£ng**: `iceberg.bronze.events_raw`
- Partition by: `event_date`
- Giá»¯ nguyÃªn dá»¯ liá»‡u gá»‘c + metadata columns

### Silver Layer (Cleaned Data)

```python
# spark/jobs/silver/clean_events.py

# LÃ m sáº¡ch: Deduplication, NULL handling, Type casting
silver_df = bronze_df
    .dropDuplicates(["event_time", "user_id", "product_id"])
    .withColumn("brand", coalesce(col("brand"), lit("Unknown")))
    .withColumn("category_level1", split(col("category_code"), "\\.")[0])
```

**Báº£ng**:
- `iceberg.silver.events_cleaned` - Events Ä‘Ã£ lÃ m sáº¡ch
- `iceberg.silver.dim_products` - Product dimension
- `iceberg.silver.dim_users` - User dimension

### Gold Layer (Business Aggregations)

```python
# spark/jobs/gold/aggregate_sales.py

# Táº¡o aggregated tables cho bÃ¡o cÃ¡o
daily_sales = silver_df
    .filter(col("event_type") == "purchase")
    .groupBy("event_date", "category_level1")
    .agg(sum("price").alias("revenue"))
```

**Báº£ng**:
- `iceberg.gold.daily_sales_by_category` - Doanh thu theo ngÃ y/category
- `iceberg.gold.funnel_analysis` - PhÃ¢n tÃ­ch funnel
- `iceberg.gold.customer_rfm` - RFM segmentation
- `iceberg.gold.product_performance` - Product metrics

---

## ğŸ”„ Schema Evolution Demo

Má»™t trong nhá»¯ng tÃ­nh nÄƒng máº¡nh máº½ cá»§a Iceberg lÃ  **Schema Evolution** - kháº£ nÄƒng thay Ä‘á»•i schema mÃ  khÃ´ng cáº§n rewrite data.

### Demo: ThÃªm cá»™t `payment_method`

```bash
# Cháº¡y demo
make demo-schema-evolution
```

```python
# NgÃ y T: Schema ban Ä‘áº§u (khÃ´ng cÃ³ payment_method)
# NgÃ y T+1: ThÃªm cá»™t má»›i
spark.sql("""
    ALTER TABLE iceberg.bronze.events_raw 
    ADD COLUMN payment_method STRING
""")

# Iceberg tá»± Ä‘á»™ng xá»­ lÃ½:
# - Dá»¯ liá»‡u cÅ©: payment_method = NULL
# - Dá»¯ liá»‡u má»›i: cÃ³ giÃ¡ trá»‹ payment_method
# - KHÃ”NG rewrite data files!
```

### Time Travel

```sql
-- Query data táº¡i snapshot cÅ©
SELECT * FROM iceberg.bronze.events_raw
VERSION AS OF 123456789;

-- Query data táº¡i thá»i Ä‘iá»ƒm cá»¥ thá»ƒ
SELECT * FROM iceberg.bronze.events_raw
TIMESTAMP AS OF '2024-01-15 10:00:00';
```

---

## ğŸ“Š Dashboards

### Superset Setup

```bash
# Tá»± Ä‘á»™ng setup
make setup-superset

# Hoáº·c cháº¡y script
python superset/setup_superset.py
```

### Charts ÄÆ°á»£c Táº¡o

1. **ğŸ“ˆ Daily Revenue Trend** (Line Chart)
   - Dataset: `daily_sales`
   - Metric: SUM(total_revenue)
   - Time grain: Day

2. **ğŸ¥§ Revenue by Category** (Pie Chart)
   - Dataset: `daily_sales`
   - Dimension: category_level1

3. **ğŸ“Š Conversion Funnel** (Funnel Chart)
   - View â†’ Cart â†’ Purchase

4. **ğŸ‘¥ Customer Segments** (Bar Chart)
   - Dataset: `customer_rfm`
   - RFM segment distribution

5. **ğŸ“‹ Top Products** (Table)
   - Dataset: `product_performance`
   - Top 10 by revenue

6. **ğŸ”¢ KPI Cards** (Big Number)
   - Total Revenue
   - Total Orders
   - Conversion Rate

### ClickHouse Queries

```sql
-- Top categories by revenue
SELECT category_level1, sum(total_revenue) as revenue
FROM lakehouse.daily_sales
GROUP BY category_level1
ORDER BY revenue DESC;

-- Customer segment distribution
SELECT customer_segment, count() as customers
FROM lakehouse.customer_rfm
GROUP BY customer_segment;

-- Conversion funnel
SELECT 
    sum(views) as views,
    sum(carts) as carts,
    sum(purchases) as purchases,
    round(sum(carts) * 100.0 / sum(views), 2) as view_to_cart_pct,
    round(sum(purchases) * 100.0 / sum(carts), 2) as cart_to_purchase_pct
FROM lakehouse.funnel_analysis;
```

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
data-lakehouse/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml      # Main orchestration
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ Dockerfile          # Spark + Iceberg image
â”‚   â”‚   â””â”€â”€ spark-defaults.conf
â”‚   â”œâ”€â”€ clickhouse/
â”‚   â”‚   â”œâ”€â”€ config.xml
â”‚   â”‚   â””â”€â”€ users.xml
â”‚   â””â”€â”€ superset/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ superset_config.py
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ jobs/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”‚   â”œâ”€â”€ ingest_events.py
â”‚       â”‚   â””â”€â”€ schema_evolution_demo.py
â”‚       â”œâ”€â”€ silver/
â”‚       â”‚   â””â”€â”€ clean_events.py
â”‚       â”œâ”€â”€ gold/
â”‚       â”‚   â””â”€â”€ aggregate_sales.py
â”‚       â””â”€â”€ serving/
â”‚           â””â”€â”€ sync_to_clickhouse.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”‚   â”œâ”€â”€ stg_events.sql
â”‚       â”‚   â”œâ”€â”€ stg_products.sql
â”‚       â”‚   â””â”€â”€ stg_users.sql
â”‚       â””â”€â”€ marts/
â”‚           â””â”€â”€ core/
â”‚               â”œâ”€â”€ fct_daily_sales.sql
â”‚               â”œâ”€â”€ fct_funnel.sql
â”‚               â””â”€â”€ dim_customer_rfm.sql
â”œâ”€â”€ clickhouse/
â”‚   â”œâ”€â”€ migrations/
â”‚   â”‚   â””â”€â”€ 001_create_iceberg_tables.sql
â”‚   â””â”€â”€ queries/
â”‚       â””â”€â”€ sample_queries.sql
â”œâ”€â”€ superset/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â””â”€â”€ dashboard_config.json
â”‚   â””â”€â”€ setup_superset.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ run_pipeline.sh
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                    # Place CSV files here
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ARCHITECTURE.md
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---

## ğŸ”§ Troubleshooting

### Service khÃ´ng khá»Ÿi Ä‘á»™ng

```bash
# Kiá»ƒm tra logs
make logs

# Kiá»ƒm tra tráº¡ng thÃ¡i
make status

# Khá»Ÿi Ä‘á»™ng láº¡i
make restart
```

### Spark job tháº¥t báº¡i

```bash
# Kiá»ƒm tra Spark UI
# http://localhost:8080

# Xem logs Spark
make logs-spark

# TÄƒng memory náº¿u cáº§n
# Chá»‰nh sá»­a docker-compose.yml: SPARK_EXECUTOR_MEMORY
```

### ClickHouse connection error

```bash
# Kiá»ƒm tra ClickHouse status
docker exec clickhouse clickhouse-client --password clickhouse123 -q "SELECT 1"

# Xem logs
make logs-clickhouse
```

### Superset khÃ´ng load dashboard

```bash
# Khá»Ÿi Ä‘á»™ng láº¡i Superset
docker compose restart superset

# Kiá»ƒm tra database connection trong Superset UI
# Settings â†’ Database Connections
```

### Out of memory

```bash
# Giáº£m sá»‘ lÆ°á»£ng partitions
# Trong spark-defaults.conf:
spark.sql.shuffle.partitions=10

# Hoáº·c xá»­ lÃ½ data theo batch nhá» hÆ¡n
```

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

1. [MinIO - Building a Data Lakehouse using Apache Iceberg](https://blog.min.io/building-a-data-lakehouse-using-apache-iceberg-and-minio/)
2. [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
3. [Apache Spark with Iceberg](https://iceberg.apache.org/docs/latest/spark-getting-started/)
4. [ClickHouse Documentation](https://clickhouse.com/docs/en/)
5. [Apache Superset Documentation](https://superset.apache.org/docs/intro)
6. [dbt Documentation](https://docs.getdbt.com/)

---


---

## ğŸ“„ License

MIT License - Xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

---

<p align="center">
  <b>ğŸš€ Happy Data Engineering! ğŸš€</b>
</p>
