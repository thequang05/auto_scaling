# ğŸ—ï¸ Data Lakehouse - Open Source Platform

> **DATAFLOW 2026: THE ALCHEMY OF MINDS**
> 
> Full-stack Open-Source Data Lakehouse Platform sá»­ dá»¥ng kiáº¿n trÃºc Medallion

[![Docker](https://img.shields.io/badge/Docker-Ready-blue?logo=docker)](https://docker.com)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange?logo=apachespark)](https://spark.apache.org)
[![Apache Iceberg](https://img.shields.io/badge/Apache%20Iceberg-1.4-blue)](https://iceberg.apache.org)
[![ClickHouse](https://img.shields.io/badge/ClickHouse-24.1-yellow?logo=clickhouse)](https://clickhouse.com)
[![Apache Superset](https://img.shields.io/badge/Apache%20Superset-3.1-cyan)](https://superset.apache.org)

---

## ğŸ“‹ Má»¥c Lá»¥c

- [Giá»›i Thiá»‡u](#-giá»›i-thiá»‡u)
- [Kiáº¿n TrÃºc Há»‡ Thá»‘ng](#-kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [CÃ´ng Nghá»‡ Sá»­ Dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [YÃªu Cáº§u Há»‡ Thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Äáº·t](#-cÃ i-Ä‘áº·t)
- [HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [Data Pipeline](#-data-pipeline)
- [Schema Evolution Demo](#-schema-evolution-demo)
- [Dashboards](#-dashboards)
- [Cáº¥u TrÃºc Dá»± Ãn](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ“– TÃ i Liá»‡u HÆ°á»›ng Dáº«n

> **ğŸ¯ Báº¯t Ä‘áº§u nhanh?** Xem [QUICK_START.md](./QUICK_START.md) - HÆ°á»›ng dáº«n cháº¡y project trong 5 phÃºt

> **ğŸ“˜ Cáº§n hÆ°á»›ng dáº«n chi tiáº¿t?** Xem [HUONG_DAN_CHAY_PROJECT.md](./HUONG_DAN_CHAY_PROJECT.md) - TÃ i liá»‡u Ä‘áº§y Ä‘á»§ tá»« setup Ä‘áº¿n troubleshooting

> **ğŸ“ Muá»‘n hiá»ƒu cáº¥u trÃºc project?** Xem [CAU_TRUC_PROJECT.md](./CAU_TRUC_PROJECT.md) - Giáº£i thÃ­ch chi tiáº¿t tá»«ng thÆ° má»¥c vÃ  file

---

## ğŸ¯ Giá»›i Thiá»‡u

### BÃ i ToÃ¡n

Dá»± Ã¡n xÃ¢y dá»±ng há»‡ thá»‘ng **Data Lakehouse** hoÃ n chá»‰nh tá»« con sá»‘ 0, sá»­ dá»¥ng cÃ¡c cÃ´ng nghá»‡ mÃ£ nguá»“n má»Ÿ Ä‘á»ƒ thay tháº¿ cÃ¡c dá»‹ch vá»¥ cloud managed:

| Cloud Service | Open-Source Alternative |
|--------------|------------------------|
| AWS S3 | **MinIO** |
| Databricks | **Apache Spark + Iceberg** |
| Snowflake | **ClickHouse** |

### Use Case: E-commerce Event History

**Dataset**: [eCommerce Events History in Cosmetics Shop](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop)

- **Quy mÃ´**: ~20 triá»‡u events
- **Loáº¡i dá»¯ liá»‡u**: User behavior (view, cart, purchase)
- **Thá»i gian**: Oct 2019 - Apr 2020

### BÃ i ToÃ¡n Nghiá»‡p Vá»¥

1. ğŸ“Š **PhÃ¢n tÃ­ch phá»…u chuyá»ƒn Ä‘á»•i** (Funnel Analysis)
2. ğŸ’° **PhÃ¢n tÃ­ch doanh thu theo thá»i gian** (Revenue Analysis)
3. ğŸ‘¥ **PhÃ¢n khÃºc khÃ¡ch hÃ ng RFM** (Customer Segmentation)

---

## ğŸ›ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

### Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VISUALIZATION LAYER                          â”‚
â”‚                         Apache Superset                             â”‚
â”‚                    (Dashboard & BI Reports)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SERVING LAYER                                â”‚
â”‚                          ClickHouse                                  â”‚
â”‚               (OLAP Database - Sub-second Queries)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRANSFORMATION LAYER                            â”‚
â”‚              dbt (data build tool) + Apache Spark                    â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚    BRONZE    â”‚â”€â”€â”€â–¶â”‚    SILVER    â”‚â”€â”€â”€â–¶â”‚     GOLD     â”‚           â”‚
â”‚  â”‚  (Raw Data)  â”‚    â”‚(Cleaned Data)â”‚    â”‚(Aggregated)  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TABLE FORMAT                                  â”‚
â”‚                       Apache Iceberg                                 â”‚
â”‚    [Schema Evolution, Time Travel, Partitioning, Z-Ordering]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STORAGE LAYER                                 â”‚
â”‚                           MinIO                                      â”‚
â”‚                (S3-Compatible Object Storage)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
CSV Files â†’ Spark â†’ Bronze (Iceberg) â†’ Silver (Iceberg) â†’ Gold (Iceberg) â†’ ClickHouse â†’ Superset
```

---

## ğŸ”§ CÃ´ng Nghá»‡ Sá»­ Dá»¥ng

| Táº§ng | CÃ´ng Nghá»‡ | PhiÃªn Báº£n | Má»¥c ÄÃ­ch |
|------|-----------|-----------|----------|
| Storage | MinIO | 2024.01 | Object Storage (S3-compatible) |
| Table Format | Apache Iceberg | 1.4.3 | ACID transactions, Schema Evolution |
| Compute | Apache Spark | 3.5.0 | Distributed Processing |
| Transformation | dbt | 1.7+ | Data Modeling |
| Serving | ClickHouse | 24.1 | OLAP Queries |
| Visualization | Apache Superset | 3.1.0 | Dashboards & BI |
| Orchestration | Docker Compose | 3.8 | Container Management |

---

## ğŸ’» YÃªu Cáº§u Há»‡ Thá»‘ng

### Hardware

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| RAM | 8 GB (Ä‘Ã£ tá»‘i Æ°u vá»›i cáº¥u hÃ¬nh lightweight) | **16 GB** (thoáº£i mÃ¡i hÆ¡n, nháº¥t lÃ  khi cháº¡y full pipeline + ClickHouse) |
| CPU | 4 cores | 8 cores |
| Disk | 20 GB | 50 GB |

### Software

- Docker Desktop 4.0+
- Docker Compose 2.0+
- Make (optional, cho automation)
- Python 3.9+ (cho Superset setup)

---

## ğŸš€ CÃ i Äáº·t & CÃ¡ch Cháº¡y Project (8GB RAM Friendly)

### BÆ°á»›c 1: Clone Repository

```bash
git clone https://github.com/thequang05/auto_scaling.git
cd auto_scaling
```

### BÆ°á»›c 2: Táº£i Dataset

**Option 1: Tá»± Ä‘á»™ng (khuyáº¿n nghá»‹)**

Cháº¡y script setup Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o thÆ° má»¥c vÃ  hÆ°á»›ng dáº«n download:

```bash
chmod +x scripts/setup.sh
./scripts/setup.sh
```

Script sáº½:
- Kiá»ƒm tra Docker/Docker Compose
- Táº¡o thÆ° má»¥c `data/raw`, `notebooks`, `logs`
- Kiá»ƒm tra disk space
- HÆ°á»›ng dáº«n download dataset tá»« Kaggle
- Build Docker images tá»± Ä‘á»™ng

**Option 2: Thá»§ cÃ´ng**

```bash
# Táº¡o thÆ° má»¥c
mkdir -p data/raw

# Download tá»« Kaggle
# https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop

# Giáº£i nÃ©n vÃ  Ä‘áº·t CSV files vÃ o data/raw/
```

### BÆ°á»›c 3: Build Images

Náº¿u Ä‘Ã£ cháº¡y `setup.sh`, bÆ°á»›c nÃ y Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n tá»± Ä‘á»™ng. Náº¿u khÃ´ng:

```bash
cd docker
docker compose build
```

```bash
cd auto_scaling
cd docker
docker compose build
```

> ğŸ’¡ **LÆ°u Ã½:** File `docker/docker-compose.yml` hiá»‡n táº¡i lÃ  **báº£n lightweight**  
> - ÄÃ£ tá»‘i Æ°u RAM cho mÃ¡y 8 GB  
> - Superset dÃ¹ng **SQLite ná»™i bá»™** lÃ m metadata DB (khÃ´ng cáº§n container PostgreSQL riÃªng)  

### BÆ°á»›c 4: Báº­t Háº¡ Táº§ng (MinIO, Iceberg, ClickHouse, Superset)

```bash
cd /Users/koiita/Downloads/auto_scaling

# Start lightweight services (khÃ´ng báº­t Spark Ä‘á»ƒ tiáº¿t kiá»‡m RAM)
docker compose up -d minio iceberg-rest clickhouse superset

# Kiá»ƒm tra nhanh
docker ps
```

**Services:**
- MinIO Console: `http://localhost:9001`
- Iceberg REST: `http://localhost:8181`
- ClickHouse: `http://localhost:8123`
- Superset: `http://localhost:8088`

---

## ğŸ“– HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### Quick Start (Theo RUNBOOK - Tá»‘i Æ¯u 8GB RAM)

**1ï¸âƒ£ Báº­t háº¡ táº§ng (MinIO, Iceberg, ClickHouse, Superset)**  
```bash
cd /Users/koiita/Downloads/auto_scaling
docker compose up -d minio iceberg-rest clickhouse superset
```

**2ï¸âƒ£ Cháº¡y Data Pipeline (Spark)**  
Do háº¡n cháº¿ RAM, Spark cháº¡y riÃªng, giá»‘ng `RUNBOOK.md`:

```bash
# Stop ClickHouse & Superset Ä‘á»ƒ giáº£i phÃ³ng RAM
docker compose stop clickhouse superset

# Start Spark
docker compose up -d spark-master spark-worker

# Cháº¡y full pipeline (Bronze â†’ Silver â†’ Gold)
./scripts/run_pipeline.sh

# Sau khi cháº¡y xong, táº¯t Spark
docker compose stop spark-master spark-worker
```

**3ï¸âƒ£ Báº­t láº¡i ClickHouse + Superset Ä‘á»ƒ xem dashboard**
```bash
docker compose up -d clickhouse superset
```

**4ï¸âƒ£ Truy cáº­p UI**
- Superset: `http://localhost:8088` (admin/admin)  
- ClickHouse: `http://localhost:8123`  

### CÃ¡c Lá»‡nh ThÆ°á»ng DÃ¹ng (Makefile)

```bash
# Infrastructure
make up              # Khá»Ÿi Ä‘á»™ng services
make down            # Dá»«ng services
make restart         # Khá»Ÿi Ä‘á»™ng láº¡i
make logs            # Xem logs
make status          # Xem tráº¡ng thÃ¡i

# Data Pipeline
make ingest-bronze        # Ingestion â†’ Bronze
make transform-silver     # Bronze â†’ Silver
make aggregate-gold       # Silver â†’ Gold
make sync-clickhouse      # Gold â†’ ClickHouse
make pipeline-full        # Cháº¡y toÃ n bá»™

# Development
make spark-shell     # Má»Ÿ Spark Shell (Scala)
make spark-pyspark   # Má»Ÿ PySpark Shell
make spark-sql       # Má»Ÿ Spark SQL
make clickhouse-client  # Má»Ÿ ClickHouse client

# dbt
make dbt-run         # Cháº¡y dbt models
make dbt-test        # Cháº¡y dbt tests
make dbt-docs        # Generate docs
```

---

## ğŸ”„ Data Pipeline

### Bronze Layer (Raw Data)

```python
# spark/jobs/bronze/ingest_events.py

# Äá»c CSV â†’ Ghi Iceberg vá»›i metadata
bronze_df = spark.read.csv("data/raw/*.csv")
bronze_df.withColumn("_ingestion_time", current_timestamp())
         .withColumn("_source_file", input_file_name())
         .writeTo("iceberg.bronze.events_raw")
         .partitionedBy("event_date")
         .create()
```

**Báº£ng**: `iceberg.bronze.events_raw`
- Partition by: `event_date`
- Giá»¯ nguyÃªn dá»¯ liá»‡u gá»‘c + metadata columns

### Silver Layer (Cleaned Data)

```python
# spark/jobs/silver/clean_events.py

# LÃ m sáº¡ch: Deduplication, NULL handling, Type casting
silver_df = bronze_df
    .dropDuplicates(["event_time", "user_id", "product_id"])
    .withColumn("brand", coalesce(col("brand"), lit("Unknown")))
    .withColumn("category_level1", split(col("category_code"), "\\.")[0])
```

**Báº£ng**:
- `iceberg.silver.events_cleaned` - Events Ä‘Ã£ lÃ m sáº¡ch
- `iceberg.silver.dim_products` - Product dimension
- `iceberg.silver.dim_users` - User dimension

### Gold Layer (Business Aggregations)

```python
# spark/jobs/gold/aggregate_sales.py

# Táº¡o aggregated tables cho bÃ¡o cÃ¡o
daily_sales = silver_df
    .filter(col("event_type") == "purchase")
    .groupBy("event_date", "category_level1")
    .agg(sum("price").alias("revenue"))
```

**Báº£ng**:
- `iceberg.gold.daily_sales_by_category` - Doanh thu theo ngÃ y/category
- `iceberg.gold.funnel_analysis` - PhÃ¢n tÃ­ch funnel
- `iceberg.gold.customer_rfm` - RFM segmentation
- `iceberg.gold.product_performance` - Product metrics

---

## ğŸ”„ Schema Evolution Demo

Má»™t trong nhá»¯ng tÃ­nh nÄƒng máº¡nh máº½ cá»§a Iceberg lÃ  **Schema Evolution** - kháº£ nÄƒng thay Ä‘á»•i schema mÃ  khÃ´ng cáº§n rewrite data.

### Demo: ThÃªm cá»™t `payment_method`

```bash
# Cháº¡y demo
make demo-schema-evolution
```

```python
# NgÃ y T: Schema ban Ä‘áº§u (khÃ´ng cÃ³ payment_method)
# NgÃ y T+1: ThÃªm cá»™t má»›i
spark.sql("""
    ALTER TABLE iceberg.bronze.events_raw 
    ADD COLUMN payment_method STRING
""")

# Iceberg tá»± Ä‘á»™ng xá»­ lÃ½:
# - Dá»¯ liá»‡u cÅ©: payment_method = NULL
# - Dá»¯ liá»‡u má»›i: cÃ³ giÃ¡ trá»‹ payment_method
# - KHÃ”NG rewrite data files!
```

### Time Travel

```sql
-- Query data táº¡i snapshot cÅ©
SELECT * FROM iceberg.bronze.events_raw
VERSION AS OF 123456789;

-- Query data táº¡i thá»i Ä‘iá»ƒm cá»¥ thá»ƒ
SELECT * FROM iceberg.bronze.events_raw
TIMESTAMP AS OF '2024-01-15 10:00:00';
```

---

## ğŸ“Š Dashboards

### Superset Setup

```bash
# Tá»± Ä‘á»™ng setup
make setup-superset

# Hoáº·c cháº¡y script
python superset/setup_superset.py
```

### Charts ÄÆ°á»£c Táº¡o

1. **ğŸ“ˆ Daily Revenue Trend** (Line Chart)
   - Dataset: `daily_sales`
   - Metric: SUM(total_revenue)
   - Time grain: Day

2. **ğŸ¥§ Revenue by Category** (Pie Chart)
   - Dataset: `daily_sales`
   - Dimension: category_level1

3. **ğŸ“Š Conversion Funnel** (Funnel Chart)
   - View â†’ Cart â†’ Purchase

4. **ğŸ‘¥ Customer Segments** (Bar Chart)
   - Dataset: `customer_rfm`
   - RFM segment distribution

5. **ğŸ“‹ Top Products** (Table)
   - Dataset: `product_performance`
   - Top 10 by revenue

6. **ğŸ”¢ KPI Cards** (Big Number)
   - Total Revenue
   - Total Orders
   - Conversion Rate

### ClickHouse Queries

```sql
-- Top categories by revenue
SELECT category_level1, sum(total_revenue) as revenue
FROM lakehouse.daily_sales
GROUP BY category_level1
ORDER BY revenue DESC;

-- Customer segment distribution
SELECT customer_segment, count() as customers
FROM lakehouse.customer_rfm
GROUP BY customer_segment;

-- Conversion funnel
SELECT 
    sum(views) as views,
    sum(carts) as carts,
    sum(purchases) as purchases,
    round(sum(carts) * 100.0 / sum(views), 2) as view_to_cart_pct,
    round(sum(purchases) * 100.0 / sum(carts), 2) as cart_to_purchase_pct
FROM lakehouse.funnel_analysis;
```

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
data-lakehouse/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml      # Main orchestration
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ Dockerfile          # Spark + Iceberg image
â”‚   â”‚   â””â”€â”€ spark-defaults.conf
â”‚   â”œâ”€â”€ clickhouse/
â”‚   â”‚   â”œâ”€â”€ config.xml
â”‚   â”‚   â””â”€â”€ users.xml
â”‚   â””â”€â”€ superset/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ superset_config.py
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ jobs/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”‚   â”œâ”€â”€ ingest_events.py
â”‚       â”‚   â””â”€â”€ schema_evolution_demo.py
â”‚       â”œâ”€â”€ silver/
â”‚       â”‚   â””â”€â”€ clean_events.py
â”‚       â”œâ”€â”€ gold/
â”‚       â”‚   â””â”€â”€ aggregate_sales.py
â”‚       â””â”€â”€ serving/
â”‚           â””â”€â”€ sync_to_clickhouse.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”‚   â”œâ”€â”€ stg_events.sql
â”‚       â”‚   â”œâ”€â”€ stg_products.sql
â”‚       â”‚   â””â”€â”€ stg_users.sql
â”‚       â””â”€â”€ marts/
â”‚           â””â”€â”€ core/
â”‚               â”œâ”€â”€ fct_daily_sales.sql
â”‚               â”œâ”€â”€ fct_funnel.sql
â”‚               â””â”€â”€ dim_customer_rfm.sql
â”œâ”€â”€ clickhouse/
â”‚   â”œâ”€â”€ migrations/
â”‚   â”‚   â””â”€â”€ 001_create_iceberg_tables.sql
â”‚   â””â”€â”€ queries/
â”‚       â””â”€â”€ sample_queries.sql
â”œâ”€â”€ superset/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â””â”€â”€ dashboard_config.json
â”‚   â””â”€â”€ setup_superset.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ run_pipeline.sh
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                    # Place CSV files here
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ARCHITECTURE.md
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---

## ğŸ”§ Troubleshooting

### Service khÃ´ng khá»Ÿi Ä‘á»™ng

```bash
# Kiá»ƒm tra logs
make logs

# Kiá»ƒm tra tráº¡ng thÃ¡i
make status

# Khá»Ÿi Ä‘á»™ng láº¡i
make restart
```

### Spark job tháº¥t báº¡i

```bash
# Kiá»ƒm tra Spark UI
# http://localhost:8080

# Xem logs Spark
make logs-spark

# TÄƒng memory náº¿u cáº§n
# Chá»‰nh sá»­a docker-compose.yml: SPARK_EXECUTOR_MEMORY
```

### ClickHouse connection error

```bash
# Kiá»ƒm tra ClickHouse status
docker exec clickhouse clickhouse-client --password clickhouse123 -q "SELECT 1"

# Xem logs
make logs-clickhouse
```

### Superset khÃ´ng load dashboard

```bash
# Khá»Ÿi Ä‘á»™ng láº¡i Superset
docker compose restart superset

# Kiá»ƒm tra database connection trong Superset UI
# Settings â†’ Database Connections
```

### Out of memory

```bash
# Giáº£m sá»‘ lÆ°á»£ng partitions
# Trong spark-defaults.conf:
spark.sql.shuffle.partitions=10

# Hoáº·c xá»­ lÃ½ data theo batch nhá» hÆ¡n
```

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

1. [MinIO - Building a Data Lakehouse using Apache Iceberg](https://blog.min.io/building-a-data-lakehouse-using-apache-iceberg-and-minio/)
2. [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
3. [Apache Spark with Iceberg](https://iceberg.apache.org/docs/latest/spark-getting-started/)
4. [ClickHouse Documentation](https://clickhouse.com/docs/en/)
5. [Apache Superset Documentation](https://superset.apache.org/docs/intro)
6. [dbt Documentation](https://docs.getdbt.com/)

---


---

## ğŸ“„ License

MIT License - Xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

---

<p align="center">
  <b>ğŸš€ Happy Data Engineering! ğŸš€</b>
</p>
