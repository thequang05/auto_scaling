"""
=============================================================================
BRONZE LAYER: Ingest Raw E-commerce Events
=============================================================================
ƒê·ªçc d·ªØ li·ªáu th√¥ t·ª´ CSV v√† ghi v√†o Iceberg tables tr√™n MinIO.
Gi·ªØ nguy√™n d·ªØ li·ªáu g·ªëc, ch·ªâ th√™m metadata columns.

Usage:
    spark-submit --master spark://spark-master:7077 ingest_events.py
=============================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, current_timestamp, input_file_name, lit,
    to_date, year, month, dayofmonth, hour
)
from pyspark.sql.types import (
    StructType, StructField, StringType, LongType, 
    DoubleType, TimestampType
)
from datetime import datetime
import os


def get_spark_session():
    """
    Kh·ªüi t·∫°o SparkSession v·ªõi c·∫•u h√¨nh Iceberg + MinIO.
    """
    spark = SparkSession.builder \
        .appName("Bronze-IngestEvents") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg.type", "rest") \
        .config("spark.sql.catalog.iceberg.uri", "http://iceberg-rest:8181") \
        .config("spark.sql.catalog.iceberg.warehouse", "s3a://iceberg-warehouse/") \
        .config("spark.sql.catalog.iceberg.io-impl", "org.apache.iceberg.hadoop.HadoopFileIO") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.sql.defaultCatalog", "iceberg") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def define_raw_schema():
    """
    Schema cho d·ªØ li·ªáu e-commerce events (REES46/Cosmetics Shop format).
    """
    return StructType([
        StructField("event_time", TimestampType(), True),
        StructField("event_type", StringType(), True),
        StructField("product_id", LongType(), True),
        StructField("category_id", LongType(), True),
        StructField("category_code", StringType(), True),
        StructField("brand", StringType(), True),
        StructField("price", DoubleType(), True),
        StructField("user_id", LongType(), True),
        StructField("user_session", StringType(), True),
    ])


def create_namespace_if_not_exists(spark, namespace: str):
    """
    T·∫°o namespace (database) n·∫øu ch∆∞a t·ªìn t·∫°i.
    """
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS iceberg.{namespace}")
    print(f"‚úì Namespace '{namespace}' ready")


def ingest_csv_to_bronze(spark, source_path: str, table_name: str):
    """
    ƒê·ªçc d·ªØ li·ªáu CSV v√† ghi v√†o Bronze Layer (Iceberg).
    
    Args:
        spark: SparkSession
        source_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file/folder CSV
        table_name: T√™n b·∫£ng Iceberg (format: namespace.table)
    """
    print(f"\n{'='*60}")
    print(f"INGESTING DATA TO BRONZE LAYER")
    print(f"{'='*60}")
    print(f"Source: {source_path}")
    print(f"Target: {table_name}")
    print(f"{'='*60}\n")
    
    # ƒê·ªçc d·ªØ li·ªáu th√¥
    raw_df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(source_path)
    
    # Th√™m metadata columns
    bronze_df = raw_df \
        .withColumn("_ingestion_time", current_timestamp()) \
        .withColumn("_source_file", input_file_name()) \
        .withColumn("_batch_id", lit(datetime.now().strftime("%Y%m%d_%H%M%S")))
    
    # Th√™m partition columns
    if "event_time" in bronze_df.columns:
        bronze_df = bronze_df \
            .withColumn("event_date", to_date(col("event_time"))) \
            .withColumn("event_year", year(col("event_time"))) \
            .withColumn("event_month", month(col("event_time")))
    
    # Show sample
    print("Sample data:")
    bronze_df.show(5, truncate=False)
    print(f"\nTotal records: {bronze_df.count():,}")
    print(f"Schema:\n{bronze_df.printSchema()}")
    
    # T·∫°o ho·∫∑c append v√†o b·∫£ng Iceberg
    bronze_df.writeTo(table_name) \
        .tableProperty("format-version", "2") \
        .tableProperty("write.parquet.compression-codec", "snappy") \
        .partitionedBy("event_date") \
        .createOrReplace()
    
    print(f"\n‚úì Data ingested successfully to {table_name}")
    
    # Verify
    verify_df = spark.table(table_name)
    print(f"‚úì Verification - Table row count: {verify_df.count():,}")
    
    return bronze_df


def show_table_metadata(spark, table_name: str):
    """
    Hi·ªÉn th·ªã metadata c·ªßa b·∫£ng Iceberg.
    """
    print(f"\n{'='*60}")
    print(f"TABLE METADATA: {table_name}")
    print(f"{'='*60}")
    
    # Snapshots
    print("\nüì∏ Snapshots:")
    spark.sql(f"SELECT * FROM {table_name}.snapshots").show(truncate=False)
    
    # Files
    print("\nüìÅ Data Files:")
    spark.sql(f"SELECT * FROM {table_name}.files LIMIT 5").show(truncate=False)
    
    # Partitions
    print("\nüìÇ Partitions:")
    spark.sql(f"SELECT * FROM {table_name}.partitions").show(truncate=False)


def main():
    """
    Main ingestion pipeline.
    """
    spark = get_spark_session()
    
    try:
        # T·∫°o namespace cho Bronze layer
        create_namespace_if_not_exists(spark, "bronze")
        
        # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu ngu·ªìn
        # C√≥ th·ªÉ l√† local path ho·∫∑c S3 path
        source_path = os.environ.get(
            "SOURCE_PATH", 
            "/opt/data/raw/*.csv"  # Default local path
        )
        
        # Alternative: Read from MinIO directly
        # source_path = "s3a://lakehouse-bronze/raw/*.csv"
        
        # Ingest events data
        ingest_csv_to_bronze(
            spark=spark,
            source_path=source_path,
            table_name="iceberg.bronze.events_raw"
        )
        
        # Show metadata
        show_table_metadata(spark, "iceberg.bronze.events_raw")
        
        print("\n" + "="*60)
        print("‚úÖ BRONZE LAYER INGESTION COMPLETED SUCCESSFULLY")
        print("="*60)
        
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        raise
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
