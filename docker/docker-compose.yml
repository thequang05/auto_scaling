version: '3.8'

# =============================================================================
# DATA LAKEHOUSE - DOCKER COMPOSE
# =============================================================================
# Services:
#   - MinIO (Object Storage)
#   - Iceberg REST Catalog
#   - Spark Master + Worker
#   - ClickHouse
#   - Superset
# =============================================================================

services:
  # ===========================================================================
  # STORAGE LAYER - MinIO (S3 Compatible)
  # ===========================================================================
  minio:
    image: quay.io/minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: minio
    hostname: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - lakehouse-network
    # healthcheck:
    #   test: ["CMD-SHELL", "exit 0"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3

  # MinIO Client - Create buckets on startup
  minio-setup:
    image: quay.io/minio/mc:RELEASE.2024-01-16T16-06-34Z
    container_name: minio-setup
    depends_on:
      minio:
        condition: service_started
    entrypoint: >
      /bin/sh -c "
      mc alias set minio http://minio:9000 minioadmin minioadmin123;
      mc mb minio/lakehouse --ignore-existing;
      mc mb minio/lakehouse-bronze --ignore-existing;
      mc mb minio/lakehouse-silver --ignore-existing;
      mc mb minio/lakehouse-gold --ignore-existing;
      mc mb minio/iceberg-warehouse --ignore-existing;
      mc anonymous set public minio/lakehouse;
      echo 'MinIO buckets created successfully!';
      exit 0;
      "
    networks:
      - lakehouse-network

  # ===========================================================================
  # CATALOG SERVICE - Iceberg REST Catalog
  # ===========================================================================
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    hostname: iceberg-rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin123
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://iceberg-warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
    depends_on:
      minio-setup:
        condition: service_completed_successfully
    networks:
      - lakehouse-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1/config"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===========================================================================
  # COMPUTE LAYER - Apache Spark
  # ===========================================================================
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    environment:
      SPARK_NO_DAEMONIZE: "true"
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin123
      AWS_REGION: us-east-1
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: "7077"
      SPARK_MASTER_WEBUI_PORT: "8080"
    volumes:
      - ../spark:/opt/spark-apps
      - ../data:/opt/data
      - spark-logs:/opt/spark/logs
    command: >
      bash -c "
        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080
      "
    depends_on:
      iceberg-rest:
        condition: service_started
    networks:
      - lakehouse-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    hostname: spark-worker
    ports:
      - "8081:8081"
    environment:
      SPARK_NO_DAEMONIZE: "true"
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin123
      AWS_REGION: us-east-1
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: "2"
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_WEBUI_PORT: "8081"
    volumes:
      - ../spark:/opt/spark-apps
      - ../data:/opt/data
      - spark-logs:/opt/spark/logs
    command: >
      bash -c "
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081
      "
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - lakehouse-network

  # Spark Thrift Server (for dbt-spark)
  spark-thrift:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-thrift
    hostname: spark-thrift
    ports:
      - "10000:10000"
      - "4041:4040"
    environment:
      SPARK_NO_DAEMONIZE: "true"
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin123
      AWS_REGION: us-east-1
      SPARK_MASTER_URL: spark://spark-master:7077
    volumes:
      - ../spark:/opt/spark-apps
      - ../data:/opt/data
      - spark-logs:/opt/spark/logs
    user: root
    command: >
      bash -c "/opt/spark/sbin/start-thriftserver.sh --master spark://spark-master:7077 --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.iceberg.type=rest --conf spark.sql.catalog.iceberg.uri=http://iceberg-rest:8181 --conf spark.sql.catalog.iceberg.warehouse=s3://iceberg-warehouse/ --conf spark.sql.catalog.iceberg.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.iceberg.s3.endpoint=http://minio:9000 --conf spark.sql.catalog.iceberg.s3.path-style-access=true --conf spark.sql.defaultCatalog=iceberg"
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - lakehouse-network

  # ===========================================================================
  # SERVING LAYER - ClickHouse
  # ===========================================================================
  clickhouse:
    image: clickhouse/clickhouse-server:24.1-alpine
    container_name: clickhouse
    hostname: clickhouse
    ports:
      - "8123:8123"
      - "9009:9000"
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: clickhouse123
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - clickhouse-logs:/var/log/clickhouse-server
      - ./clickhouse/config.xml:/etc/clickhouse-server/config.d/custom.xml:ro
      - ./clickhouse/users.xml:/etc/clickhouse-server/users.d/custom.xml:ro
    networks:
      - lakehouse-network
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      minio-setup:
        condition: service_completed_successfully

  # ===========================================================================
  # VISUALIZATION LAYER - Apache Superset
  # ===========================================================================
  superset-db:
    image: postgres:15-alpine
    container_name: superset-db
    hostname: superset-db
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset123
      POSTGRES_DB: superset
    volumes:
      - superset-db-data:/var/lib/postgresql/data
    networks:
      - lakehouse-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset"]
      interval: 10s
      timeout: 5s
      retries: 5

  superset-cache:
    image: redis:7-alpine
    container_name: superset-cache
    hostname: superset-cache
    volumes:
      - superset-cache-data:/data
    networks:
      - lakehouse-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    container_name: superset
    hostname: superset
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: supersecretkey123456789
      DATABASE_URL: postgresql://superset:superset123@superset-db:5432/superset
      REDIS_URL: redis://superset-cache:6379/0
      FLASK_ENV: development
      SUPERSET_LOAD_EXAMPLES: "no"
    volumes:
      - ./superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - superset-home:/app/superset_home
    networks:
      - lakehouse-network
    depends_on:
      superset-db:
        condition: service_healthy
      superset-cache:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 10

  # Superset initialization
  superset-init:
    build:
      context: ./superset
      dockerfile: Dockerfile
    container_name: superset-init
    environment:
      SUPERSET_SECRET_KEY: supersecretkey123456789
      DATABASE_URL: postgresql://superset:superset123@superset-db:5432/superset
      REDIS_URL: redis://superset-cache:6379/0
    volumes:
      - ./superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - superset-home:/app/superset_home
    networks:
      - lakehouse-network
    depends_on:
      superset-db:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin ||
        echo 'Admin already exists' &&
        superset init &&
        echo 'Superset initialized successfully!'
      "

# ===========================================================================
# NETWORKS
# ===========================================================================
networks:
  lakehouse-network:
    driver: bridge
    name: lakehouse-network

# ===========================================================================
# VOLUMES
# ===========================================================================
volumes:
  minio-data:
    name: lakehouse-minio-data
  spark-logs:
    name: lakehouse-spark-logs
  clickhouse-data:
    name: lakehouse-clickhouse-data
  clickhouse-logs:
    name: lakehouse-clickhouse-logs
  superset-db-data:
    name: lakehouse-superset-db
  superset-cache-data:
    name: lakehouse-superset-cache
  superset-home:
    name: lakehouse-superset-home
