# üìò H∆Ø·ªöNG D·∫™N CH·∫†Y PROJECT DATA LAKEHOUSE

> **T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch setup v√† ch·∫°y h·ªá th·ªëng Data Lakehouse t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi**

---

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan D·ª± √Ån](#1-t·ªïng-quan-d·ª±-√°n)
2. [Y√™u C·∫ßu H·ªá Th·ªëng](#2-y√™u-c·∫ßu-h·ªá-th·ªëng)
3. [C√†i ƒê·∫∑t Ban ƒê·∫ßu](#3-c√†i-ƒë·∫∑t-ban-ƒë·∫ßu)
4. [Chu·∫©n B·ªã D·ªØ Li·ªáu](#4-chu·∫©n-b·ªã-d·ªØ-li·ªáu)
5. [Kh·ªüi ƒê·ªông H·ªá Th·ªëng](#5-kh·ªüi-ƒë·ªông-h·ªá-th·ªëng)
6. [Ch·∫°y Data Pipeline](#6-ch·∫°y-data-pipeline)
7. [Thi·∫øt L·∫≠p Superset Dashboard](#7-thi·∫øt-l·∫≠p-superset-dashboard)
8. [Ki·ªÉm Tra K·∫øt Qu·∫£](#8-ki·ªÉm-tra-k·∫øt-qu·∫£)
9. [C√°c L·ªánh Th∆∞·ªùng D√πng](#9-c√°c-l·ªánh-th∆∞·ªùng-d√πng)
10. [Troubleshooting](#10-troubleshooting)

---

## 1. T·ªïng Quan D·ª± √Ån

### 1.1. M·ª•c ƒê√≠ch

D·ª± √°n x√¢y d·ª±ng m·ªôt **Data Lakehouse Platform** ho√†n ch·ªânh s·ª≠ d·ª•ng ki·∫øn tr√∫c **Medallion** (Bronze ‚Üí Silver ‚Üí Gold) v·ªõi c√°c c√¥ng ngh·ªá m√£ ngu·ªìn m·ªü:

- **Storage**: MinIO (thay th·∫ø AWS S3)
- **Table Format**: Apache Iceberg (ACID transactions, Schema Evolution)
- **Compute**: Apache Spark (x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n)
- **Serving**: ClickHouse (OLAP database)
- **Visualization**: Apache Superset (BI dashboards)

### 1.2. Use Case

X·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu **E-commerce Events** t·ª´ Kaggle:
- Dataset: [eCommerce Events History in Cosmetics Shop](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop)
- Quy m√¥: ~20 tri·ªáu events
- Th·ªùi gian: Oct 2019 - Apr 2020

### 1.3. Ki·∫øn Tr√∫c H·ªá Th·ªëng

```
CSV Files ‚Üí Spark ‚Üí Bronze (Iceberg) ‚Üí Silver (Iceberg) ‚Üí Gold (Iceberg) ‚Üí ClickHouse ‚Üí Superset
```

**C√°c t·∫ßng d·ªØ li·ªáu:**
- **Bronze**: D·ªØ li·ªáu th√¥ (raw data) - gi·ªØ nguy√™n format g·ªëc
- **Silver**: D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch (cleaned data) - deduplication, type casting
- **Gold**: D·ªØ li·ªáu nghi·ªáp v·ª• (business metrics) - aggregations, KPIs

---

## 2. Y√™u C·∫ßu H·ªá Th·ªëng

### 2.1. Ph·∫ßn C·ª©ng

| Resource | T·ªëi Thi·ªÉu | Khuy·∫øn Ngh·ªã |
|----------|-----------|-------------|
| RAM | 8 GB | **16 GB** |
| CPU | 4 cores | 8 cores |
| Disk | 20 GB | 50 GB |

### 2.2. Ph·∫ßn M·ªÅm

- **Docker Desktop** 4.0+ (ho·∫∑c Docker Engine + Docker Compose)
- **Docker Compose** 2.0+
- **Make** (optional, nh∆∞ng khuy·∫øn ngh·ªã)
- **Python** 3.9+ (cho Superset setup script)

### 2.3. Ki·ªÉm Tra Y√™u C·∫ßu

```bash
# Ki·ªÉm tra Docker
docker --version
docker-compose --version

# Ki·ªÉm tra Make (optional)
make --version

# Ki·ªÉm tra Python
python3 --version
```

---

## 3. C√†i ƒê·∫∑t Ban ƒê·∫ßu

### 3.1. Clone Repository

```bash
# N·∫øu ch∆∞a c√≥, clone repository
git clone <repository-url>
cd auto_scaling
```

### 3.2. Ch·∫°y Setup Script (Khuy·∫øn Ngh·ªã)

Script t·ª± ƒë·ªông ki·ªÉm tra m√¥i tr∆∞·ªùng v√† t·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt:

```bash
chmod +x scripts/setup.sh
./scripts/setup.sh
```

Script s·∫Ω:
- ‚úÖ Ki·ªÉm tra Docker v√† Docker Compose
- ‚úÖ Ki·ªÉm tra t√†i nguy√™n h·ªá th·ªëng (RAM, disk)
- ‚úÖ T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt (`data/raw`, `logs`, etc.)
- ‚úÖ Build Docker images

### 3.3. Build Docker Images Th·ªß C√¥ng

N·∫øu kh√¥ng d√πng setup script:

```bash
cd docker
docker-compose build
```

**L∆∞u √Ω:** L·∫ßn ƒë·∫ßu build c√≥ th·ªÉ m·∫•t 10-15 ph√∫t ƒë·ªÉ download c√°c images v√† dependencies.

---

## 4. Chu·∫©n B·ªã D·ªØ Li·ªáu

### 4.1. Download Dataset

1. Truy c·∫≠p: https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop
2. Download dataset (c·∫ßn t√†i kho·∫£n Kaggle)
3. Gi·∫£i n√©n file ZIP

### 4.2. ƒê·∫∑t D·ªØ Li·ªáu V√†o Th∆∞ M·ª•c

```bash
# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
mkdir -p data/raw

# Copy c√°c file CSV v√†o th∆∞ m·ª•c data/raw/
# V√≠ d·ª•:
cp ~/Downloads/ecommerce-events-history-in-cosmetics-shop/*.csv data/raw/
```

**C√°c file CSV c·∫ßn c√≥:**
- `2019-Oct.csv`
- `2019-Nov.csv`
- `2019-Dec.csv`
- `2020-Jan.csv`
- `2020-Feb.csv`
- (v√† c√°c file kh√°c n·∫øu c√≥)

### 4.3. Ki·ªÉm Tra D·ªØ Li·ªáu

```bash
# Ki·ªÉm tra s·ªë l∆∞·ª£ng file
ls -lh data/raw/*.csv

# Ki·ªÉm tra k√≠ch th∆∞·ªõc (n√™n c√≥ v√†i GB)
du -sh data/raw/
```

---

## 5. Kh·ªüi ƒê·ªông H·ªá Th·ªëng

### 5.1. Kh·ªüi ƒê·ªông T·∫•t C·∫£ Services

**C√°ch 1: S·ª≠ d·ª•ng Make (Khuy·∫øn Ngh·ªã)**

```bash
# T·ª´ th∆∞ m·ª•c g·ªëc c·ªßa project
make up
```

**C√°ch 2: S·ª≠ d·ª•ng Docker Compose**

```bash
cd docker
docker-compose up -d
```

### 5.2. Ki·ªÉm Tra Services ƒê√£ Kh·ªüi ƒê·ªông

```bash
# Xem tr·∫°ng th√°i t·∫•t c·∫£ containers
make status

# Ho·∫∑c
cd docker
docker-compose ps
```

**C√°c services c·∫ßn ch·∫°y:**
- ‚úÖ `minio` - Object Storage
- ‚úÖ `iceberg-rest` - Iceberg REST Catalog
- ‚úÖ `spark-master` - Spark Master
- ‚úÖ `spark-worker` - Spark Worker
- ‚úÖ `spark-thrift` - Spark Thrift Server (cho dbt)
- ‚úÖ `clickhouse` - ClickHouse Database
- ‚úÖ `superset-db` - PostgreSQL cho Superset
- ‚úÖ `superset-cache` - Redis cho Superset
- ‚úÖ `superset` - Apache Superset

### 5.3. ƒê·ª£i Services Kh·ªüi ƒê·ªông Ho√†n T·∫•t

**Quan tr·ªçng:** ƒê·ª£i 1-2 ph√∫t ƒë·ªÉ t·∫•t c·∫£ services kh·ªüi ƒë·ªông ho√†n to√†n.

Ki·ªÉm tra logs:

```bash
# Xem logs t·∫•t c·∫£ services
make logs

# Ho·∫∑c xem logs t·ª´ng service
make logs-spark
make logs-clickhouse
make logs-superset
```

### 5.4. Truy C·∫≠p Web UIs

Sau khi services kh·ªüi ƒë·ªông, truy c·∫≠p c√°c URL sau:

| Service | URL | Credentials |
|---------|-----|-------------|
| **MinIO Console** | http://localhost:9001 | `minioadmin` / `minioadmin123` |
| **Spark Master UI** | http://localhost:8080 | - |
| **ClickHouse HTTP** | http://localhost:8123 | `default` / `clickhouse123` |
| **Superset** | http://localhost:8088 | `admin` / `admin` |
| **Iceberg REST** | http://localhost:8181 | - |

**Ki·ªÉm tra nhanh:**
- MinIO: M·ªü http://localhost:9001, ƒëƒÉng nh·∫≠p v√† ki·ªÉm tra bucket `lakehouse` ƒë√£ ƒë∆∞·ª£c t·∫°o
- Spark: M·ªü http://localhost:8080, ki·ªÉm tra c√≥ 1 worker ƒëang ch·∫°y
- ClickHouse: Ch·∫°y `make clickhouse-client` v√† test query `SELECT 1`

---

## 6. Ch·∫°y Data Pipeline

### 6.1. Ch·∫°y To√†n B·ªô Pipeline (Khuy·∫øn Ngh·ªã)

**C√°ch 1: S·ª≠ d·ª•ng Make**

```bash
make pipeline-full
```

L·ªánh n√†y s·∫Ω ch·∫°y tu·∫ßn t·ª±:
1. ‚úÖ Bronze Layer - Ingestion
2. ‚úÖ Silver Layer - Transformation
3. ‚úÖ Gold Layer - Aggregation
4. ‚úÖ ClickHouse - Create Tables
5. ‚úÖ Sync to ClickHouse

**C√°ch 2: S·ª≠ d·ª•ng Script**

```bash
chmod +x scripts/run_pipeline.sh
./scripts/run_pipeline.sh
```

### 6.2. Ch·∫°y T·ª´ng B∆∞·ªõc Ri√™ng L·∫ª

N·∫øu mu·ªën ch·∫°y t·ª´ng b∆∞·ªõc ƒë·ªÉ ki·ªÉm tra:

```bash
# B∆∞·ªõc 1: Ingest d·ªØ li·ªáu th√¥ v√†o Bronze
make ingest-bronze

# B∆∞·ªõc 2: Transform Bronze ‚Üí Silver
make transform-silver

# B∆∞·ªõc 3: Aggregate Silver ‚Üí Gold
make aggregate-gold

# B∆∞·ªõc 4: Sync Gold ‚Üí ClickHouse
make sync-clickhouse
```

### 6.3. Ki·ªÉm Tra Ti·∫øn ƒê·ªô

Trong khi pipeline ch·∫°y, b·∫°n c√≥ th·ªÉ:

1. **Xem Spark UI**: http://localhost:8080
   - Xem c√°c jobs ƒëang ch·∫°y
   - Xem executor status
   - Xem logs c·ªßa t·ª´ng task

2. **Xem Logs**:
   ```bash
   make logs-spark
   ```

3. **Ki·ªÉm Tra MinIO**:
   - M·ªü http://localhost:9001
   - V√†o bucket `iceberg-warehouse`
   - Ki·ªÉm tra c√°c th∆∞ m·ª•c `bronze/`, `silver/`, `gold/`

### 6.4. Th·ªùi Gian Ch·∫°y D·ª± Ki·∫øn

V·ªõi dataset ~20 tri·ªáu events:
- **Bronze Ingestion**: 5-10 ph√∫t
- **Silver Transformation**: 10-15 ph√∫t
- **Gold Aggregation**: 5-10 ph√∫t
- **ClickHouse Sync**: 5-10 ph√∫t

**T·ªïng c·ªông**: ~30-45 ph√∫t (t√πy v√†o hardware)

---

## 7. Thi·∫øt L·∫≠p Superset Dashboard

### 7.1. Setup Superset

Sau khi pipeline ch·∫°y xong, setup Superset ƒë·ªÉ k·∫øt n·ªëi v·ªõi ClickHouse:

```bash
# C√†i ƒë·∫∑t Python dependencies (n·∫øu ch∆∞a c√≥)
pip install requests

# Ch·∫°y setup script
make setup-superset

# Ho·∫∑c ch·∫°y tr·ª±c ti·∫øp
python superset/setup_superset.py
```

Script s·∫Ω:
- ‚úÖ T·∫°o database connection ƒë·∫øn ClickHouse
- ‚úÖ Import dashboard templates (n·∫øu c√≥)
- ‚úÖ T·∫°o c√°c charts m·∫´u

### 7.2. Truy C·∫≠p Superset

1. M·ªü http://localhost:8088
2. ƒêƒÉng nh·∫≠p: `admin` / `admin`
3. V√†o **Settings** ‚Üí **Database Connections**
4. Ki·ªÉm tra connection `ClickHouse Lakehouse` ƒë√£ ƒë∆∞·ª£c t·∫°o

### 7.3. T·∫°o Dashboard Th·ªß C√¥ng (N·∫øu C·∫ßn)

N·∫øu setup script kh√¥ng t·ª± ƒë·ªông t·∫°o dashboard, b·∫°n c√≥ th·ªÉ t·∫°o th·ªß c√¥ng:

1. **T·∫°o Dataset**:
   - V√†o **Data** ‚Üí **Datasets**
   - Click **+ Dataset**
   - Ch·ªçn database `ClickHouse Lakehouse`
   - Ch·ªçn table `daily_sales`, `funnel_analysis`, `customer_rfm`, etc.

2. **T·∫°o Charts**:
   - V√†o **Charts** ‚Üí **+ Chart**
   - Ch·ªçn dataset v√† t·∫°o c√°c charts:
     - **Line Chart**: Daily Revenue Trend
     - **Pie Chart**: Revenue by Category
     - **Funnel Chart**: Conversion Funnel
     - **Bar Chart**: Customer Segments
     - **Table**: Top Products

3. **T·∫°o Dashboard**:
   - V√†o **Dashboards** ‚Üí **+ Dashboard**
   - Th√™m c√°c charts v√†o dashboard

---

## 8. Ki·ªÉm Tra K·∫øt Qu·∫£

### 8.1. Ki·ªÉm Tra Iceberg Tables

**S·ª≠ d·ª•ng Spark SQL:**

```bash
make spark-sql
```

Trong Spark SQL shell:

```sql
-- Li·ªát k√™ databases
SHOW DATABASES;

-- Li·ªát k√™ tables trong Bronze
SHOW TABLES IN iceberg.bronze;

-- Li·ªát k√™ tables trong Silver
SHOW TABLES IN iceberg.silver;

-- Li·ªát k√™ tables trong Gold
SHOW TABLES IN iceberg.gold;

-- Query d·ªØ li·ªáu Bronze
SELECT COUNT(*) FROM iceberg.bronze.events_raw;

-- Query d·ªØ li·ªáu Silver
SELECT COUNT(*) FROM iceberg.silver.events_cleaned;

-- Query d·ªØ li·ªáu Gold
SELECT * FROM iceberg.gold.daily_sales_by_category LIMIT 10;
```

### 8.2. Ki·ªÉm Tra ClickHouse Tables

**S·ª≠ d·ª•ng ClickHouse Client:**

```bash
make clickhouse-client
```

Trong ClickHouse client:

```sql
-- Li·ªát k√™ databases
SHOW DATABASES;

-- Li·ªát k√™ tables
SHOW TABLES FROM lakehouse;

-- Query d·ªØ li·ªáu
SELECT COUNT(*) FROM lakehouse.daily_sales;

-- Top categories by revenue
SELECT 
    category_level1, 
    SUM(total_revenue) as revenue
FROM lakehouse.daily_sales
GROUP BY category_level1
ORDER BY revenue DESC
LIMIT 10;

-- Conversion funnel
SELECT 
    SUM(views) as views,
    SUM(carts) as carts,
    SUM(purchases) as purchases,
    ROUND(SUM(carts) * 100.0 / SUM(views), 2) as view_to_cart_pct,
    ROUND(SUM(purchases) * 100.0 / SUM(carts), 2) as cart_to_purchase_pct
FROM lakehouse.funnel_analysis;
```

### 8.3. Ki·ªÉm Tra MinIO Storage

1. M·ªü http://localhost:9001
2. ƒêƒÉng nh·∫≠p: `minioadmin` / `minioadmin123`
3. V√†o bucket `iceberg-warehouse`
4. Ki·ªÉm tra c√°c th∆∞ m·ª•c:
   - `bronze/` - ch·ª©a Bronze tables
   - `silver/` - ch·ª©a Silver tables
   - `gold/` - ch·ª©a Gold tables

### 8.4. Ki·ªÉm Tra Superset Dashboards

1. M·ªü http://localhost:8088
2. V√†o **Dashboards**
3. M·ªü dashboard ƒë√£ t·∫°o
4. Ki·ªÉm tra c√°c charts hi·ªÉn th·ªã ƒë√∫ng d·ªØ li·ªáu

---

## 9. C√°c L·ªánh Th∆∞·ªùng D√πng

### 9.1. Infrastructure Commands

```bash
# Kh·ªüi ƒë·ªông services
make up

# D·ª´ng services
make down

# Kh·ªüi ƒë·ªông l·∫°i services
make restart

# Xem logs
make logs

# Xem logs t·ª´ng service
make logs-spark
make logs-clickhouse
make logs-superset

# Xem tr·∫°ng th√°i
make status

# D·ªçn d·∫πp (x√≥a containers, volumes)
make clean
```

### 9.2. Data Pipeline Commands

```bash
# Ch·∫°y to√†n b·ªô pipeline
make pipeline-full

# Ch·∫°y t·ª´ng b∆∞·ªõc
make ingest-bronze        # CSV ‚Üí Bronze
make transform-silver     # Bronze ‚Üí Silver
make aggregate-gold       # Silver ‚Üí Gold
make sync-clickhouse      # Gold ‚Üí ClickHouse
```

### 9.3. Development Commands

```bash
# Spark Shells
make spark-shell          # Scala shell
make spark-pyspark        # PySpark shell
make spark-sql            # Spark SQL shell

# ClickHouse
make clickhouse-client    # ClickHouse client

# dbt
make dbt-run              # Ch·∫°y dbt models
make dbt-test             # Ch·∫°y dbt tests
make dbt-docs             # Generate docs
```

### 9.4. Superset Commands

```bash
# Setup Superset
make setup-superset
```

### 9.5. Demo Commands

```bash
# Demo Schema Evolution
make demo-schema-evolution

# Quick start (build + up)
make quickstart

# Full demo (build + up + pipeline + setup)
make demo
```

---

## 10. Troubleshooting

### 10.1. Services Kh√¥ng Kh·ªüi ƒê·ªông

**V·∫•n ƒë·ªÅ:** Services kh√¥ng start ho·∫∑c crash ngay sau khi start.

**Gi·∫£i ph√°p:**

```bash
# 1. Ki·ªÉm tra logs
make logs

# 2. Ki·ªÉm tra t√†i nguy√™n
docker stats

# 3. Ki·ªÉm tra ports ƒë√£ b·ªã chi·∫øm
lsof -i :8080  # Spark
lsof -i :8123  # ClickHouse
lsof -i :8088  # Superset
lsof -i :9000  # MinIO

# 4. D·ªçn d·∫πp v√† kh·ªüi ƒë·ªông l·∫°i
make clean
make up
```

### 10.2. Spark Jobs Th·∫•t B·∫°i

**V·∫•n ƒë·ªÅ:** Spark jobs fail v·ªõi l·ªói OutOfMemory ho·∫∑c timeout.

**Gi·∫£i ph√°p:**

1. **TƒÉng memory trong docker-compose.yml:**
   ```yaml
   spark-worker:
     environment:
       SPARK_WORKER_MEMORY: 4g  # TƒÉng t·ª´ 2g l√™n 4g
   ```

2. **Gi·∫£m s·ªë partitions:**
   - Ch·ªânh s·ª≠a `spark/jobs/*.py`
   - Th√™m: `.coalesce(10)` sau c√°c transformations l·ªõn

3. **Xem logs chi ti·∫øt:**
   ```bash
   make logs-spark
   # Ho·∫∑c xem Spark UI: http://localhost:8080
   ```

### 10.3. ClickHouse Connection Error

**V·∫•n ƒë·ªÅ:** Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c ClickHouse ho·∫∑c query b·ªã l·ªói.

**Gi·∫£i ph√°p:**

```bash
# 1. Ki·ªÉm tra ClickHouse ƒëang ch·∫°y
docker ps | grep clickhouse

# 2. Test connection
docker exec clickhouse clickhouse-client \
    --password clickhouse123 \
    -q "SELECT 1"

# 3. Xem logs
make logs-clickhouse

# 4. Kh·ªüi ƒë·ªông l·∫°i
docker-compose restart clickhouse
```

### 10.4. Superset Kh√¥ng Load Dashboard

**V·∫•n ƒë·ªÅ:** Superset kh√¥ng hi·ªÉn th·ªã d·ªØ li·ªáu ho·∫∑c connection error.

**Gi·∫£i ph√°p:**

1. **Ki·ªÉm tra database connection:**
   - V√†o http://localhost:8088
   - Settings ‚Üí Database Connections
   - Test connection `ClickHouse Lakehouse`

2. **Kh·ªüi ƒë·ªông l·∫°i Superset:**
   ```bash
   docker-compose restart superset
   ```

3. **Ki·ªÉm tra ClickHouse tables:**
   ```bash
   make clickhouse-client
   # Ch·∫°y: SHOW TABLES FROM lakehouse;
   ```

### 10.5. MinIO Connection Error

**V·∫•n ƒë·ªÅ:** Spark kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c MinIO.

**Gi·∫£i ph√°p:**

```bash
# 1. Ki·ªÉm tra MinIO ƒëang ch·∫°y
docker ps | grep minio

# 2. Test MinIO t·ª´ Spark container
docker exec spark-master curl http://minio:9000/minio/health/live

# 3. Ki·ªÉm tra buckets
# M·ªü http://localhost:9001 v√† ki·ªÉm tra buckets ƒë√£ ƒë∆∞·ª£c t·∫°o
```

### 10.6. Out of Disk Space

**V·∫•n ƒë·ªÅ:** H·∫øt dung l∆∞·ª£ng disk.

**Gi·∫£i ph√°p:**

```bash
# 1. Xem dung l∆∞·ª£ng ƒë√£ d√πng
docker system df

# 2. D·ªçn d·∫πp Docker
docker system prune -a

# 3. X√≥a volumes c≈© (C·∫®N TH·∫¨N: s·∫Ω m·∫•t d·ªØ li·ªáu)
make clean
```

### 10.7. Iceberg REST Catalog Error

**V·∫•n ƒë·ªÅ:** Spark kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Iceberg REST Catalog.

**Gi·∫£i ph√°p:**

```bash
# 1. Ki·ªÉm tra iceberg-rest ƒëang ch·∫°y
docker ps | grep iceberg-rest

# 2. Test REST API
curl http://localhost:8181/v1/config

# 3. Xem logs
docker logs iceberg-rest

# 4. Kh·ªüi ƒë·ªông l·∫°i
docker-compose restart iceberg-rest
```

### 10.8. D·ªØ Li·ªáu Kh√¥ng Hi·ªÉn Th·ªã Trong Superset

**V·∫•n ƒë·ªÅ:** Tables c√≥ d·ªØ li·ªáu nh∆∞ng Superset kh√¥ng query ƒë∆∞·ª£c.

**Gi·∫£i ph√°p:**

1. **Ki·ªÉm tra table permissions trong ClickHouse:**
   ```sql
   -- Trong ClickHouse client
   SELECT * FROM system.tables WHERE database = 'lakehouse';
   ```

2. **Refresh dataset trong Superset:**
   - V√†o Data ‚Üí Datasets
   - Click v√†o dataset
   - Click "Sync columns from source"

3. **Ki·ªÉm tra SQL query:**
   - T·∫°o chart m·ªõi
   - Xem SQL query ƒë∆∞·ª£c generate
   - Test query tr·ª±c ti·∫øp trong ClickHouse

---

## 11. T√†i Li·ªáu Tham Kh·∫£o

### 11.1. T√†i Li·ªáu Ch√≠nh Th·ª©c

- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [ClickHouse Documentation](https://clickhouse.com/docs/en/)
- [Apache Superset Documentation](https://superset.apache.org/docs/intro)
- [dbt Documentation](https://docs.getdbt.com/)

### 11.2. T√†i Li·ªáu Trong Project

- `README.md` - T·ªïng quan d·ª± √°n
- `docs/ARCHITECTURE.md` - Ki·∫øn tr√∫c chi ti·∫øt
- `Makefile` - Danh s√°ch t·∫•t c·∫£ commands

### 11.3. Dataset

- [Kaggle Dataset](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop)

---

## 12. Li√™n H·ªá & H·ªó Tr·ª£

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c:

1. Ki·ªÉm tra logs: `make logs`
2. Xem troubleshooting section ·ªü tr√™n
3. Ki·ªÉm tra GitHub Issues (n·∫øu c√≥)
4. Li√™n h·ªá team qua email: hamic@hus.edu.vn

---

## üìù Ghi Ch√∫ Quan Tr·ªçng

1. **L·∫ßn ƒë·∫ßu ch·∫°y:** Build Docker images c√≥ th·ªÉ m·∫•t 10-15 ph√∫t
2. **T√†i nguy√™n:** ƒê·∫£m b·∫£o c√≥ ƒë·ªß RAM (khuy·∫øn ngh·ªã 16GB)
3. **D·ªØ li·ªáu:** Dataset c·∫ßn ƒë∆∞·ª£c ƒë·∫∑t trong `data/raw/` tr∆∞·ªõc khi ch·∫°y pipeline
4. **Th·ªùi gian:** Pipeline ƒë·∫ßy ƒë·ªß m·∫•t ~30-45 ph√∫t t√πy hardware
5. **Ports:** ƒê·∫£m b·∫£o c√°c ports 8080, 8088, 8123, 9000, 9001 kh√¥ng b·ªã chi·∫øm

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi Data Lakehouse! üöÄ**
