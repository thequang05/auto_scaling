# ğŸ“ Cáº¥u TrÃºc Project - Data Lakehouse

> **TÃ i liá»‡u giáº£i thÃ­ch cáº¥u trÃºc thÆ° má»¥c vÃ  chá»©c nÄƒng cá»§a tá»«ng component**

---

## ğŸ“‚ Cáº¥u TrÃºc Tá»•ng Quan

```
auto_scaling/
â”œâ”€â”€ ğŸ“ docker/              # Docker configuration
â”œâ”€â”€ ğŸ“ spark/               # Spark jobs & applications
â”œâ”€â”€ ğŸ“ dbt/                 # dbt models & transformations
â”œâ”€â”€ ğŸ“ clickhouse/          # ClickHouse migrations & queries
â”œâ”€â”€ ğŸ“ superset/            # Superset dashboards & setup
â”œâ”€â”€ ğŸ“ data/                # Raw data files
â”œâ”€â”€ ğŸ“ scripts/             # Utility scripts
â”œâ”€â”€ ğŸ“ docs/                # Documentation
â”œâ”€â”€ ğŸ“„ Makefile             # Automation commands
â”œâ”€â”€ ğŸ“„ README.md            # Project overview
â”œâ”€â”€ ğŸ“„ QUICK_START.md       # Quick start guide
â”œâ”€â”€ ğŸ“„ HUONG_DAN_CHAY_PROJECT.md  # Full guide (Vietnamese)
â””â”€â”€ ğŸ“„ CAU_TRUC_PROJECT.md  # This file
```

---

## ğŸ“ Chi Tiáº¿t Tá»«ng ThÆ° Má»¥c

### 1. `docker/` - Docker Configuration

Chá»©a táº¥t cáº£ cáº¥u hÃ¬nh Docker vÃ  Docker Compose.

```
docker/
â”œâ”€â”€ docker-compose.yml      # Main orchestration file
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ Dockerfile          # Custom Spark image vá»›i Iceberg
â”‚   â””â”€â”€ spark-defaults.conf # Spark configuration
â”œâ”€â”€ clickhouse/
â”‚   â”œâ”€â”€ config.xml          # ClickHouse server config
â”‚   â””â”€â”€ users.xml           # ClickHouse users & permissions
â””â”€â”€ superset/
    â”œâ”€â”€ Dockerfile          # Custom Superset image
    â””â”€â”€ superset_config.py  # Superset configuration
```

**Chá»©c nÄƒng:**
- Äá»‹nh nghÄ©a táº¥t cáº£ services (MinIO, Spark, ClickHouse, Superset, etc.)
- Cáº¥u hÃ¬nh networking vÃ  volumes
- Setup environment variables

---

### 2. `spark/` - Spark Jobs

Chá»©a táº¥t cáº£ Spark applications theo kiáº¿n trÃºc Medallion.

```
spark/
â””â”€â”€ jobs/
    â”œâ”€â”€ bronze/
    â”‚   â”œâ”€â”€ ingest_events.py          # CSV â†’ Bronze Layer
    â”‚   â”œâ”€â”€ ingest_events_test.py     # Test version
    â”‚   â”œâ”€â”€ schema_evolution_demo.py  # Demo schema evolution
    â”‚   â””â”€â”€ time_travel_demo.py       # Demo time travel
    â”œâ”€â”€ silver/
    â”‚   â””â”€â”€ clean_events.py        # Bronze â†’ Silver (cleaning)
    â”œâ”€â”€ gold/
    â”‚   â””â”€â”€ aggregate_sales.py        # Silver â†’ Gold (aggregations)
    â””â”€â”€ serving/
        â””â”€â”€ sync_to_clickhouse.py     # Gold â†’ ClickHouse
```

**Chá»©c nÄƒng:**
- **Bronze**: Ingest raw data tá»« CSV vÃ o Iceberg
- **Silver**: Clean vÃ  transform data
- **Gold**: Táº¡o business metrics vÃ  aggregations
- **Serving**: Sync data vÃ o ClickHouse cho query nhanh

**CÃ¡ch cháº¡y:**
```bash
make ingest-bronze        # Cháº¡y bronze job
make transform-silver     # Cháº¡y silver job
make aggregate-gold      # Cháº¡y gold job
make sync-clickhouse     # Sync to ClickHouse
```

---

### 3. `dbt/` - Data Build Tool

Chá»©a dbt models Ä‘á»ƒ transform data tá»« Silver â†’ Gold.

```
dbt/
â”œâ”€â”€ dbt_project.yml       # dbt project configuration
â”œâ”€â”€ profiles.yml          # Connection profiles
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/          # Bronze â†’ Silver models
â”‚   â”‚   â”œâ”€â”€ stg_events.sql
â”‚   â”‚   â”œâ”€â”€ stg_products.sql
â”‚   â”‚   â””â”€â”€ stg_users.sql
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â””â”€â”€ core/         # Silver â†’ Gold models
â”‚   â”‚       â”œâ”€â”€ fct_daily_sales.sql
â”‚   â”‚       â”œâ”€â”€ fct_funnel.sql
â”‚   â”‚       â””â”€â”€ dim_customer_rfm.sql
â”‚   â””â”€â”€ schema.yml        # Model documentation
â””â”€â”€ macros/
    â””â”€â”€ utils.sql         # Reusable macros
```

**Chá»©c nÄƒng:**
- Transform data tá»« Silver layer
- Táº¡o business metrics (RFM, Funnel, etc.)
- Data quality tests
- Documentation

**CÃ¡ch cháº¡y:**
```bash
make dbt-run      # Cháº¡y táº¥t cáº£ models
make dbt-test     # Cháº¡y tests
make dbt-docs     # Generate documentation
```

---

### 4. `clickhouse/` - ClickHouse

Chá»©a migrations vÃ  sample queries cho ClickHouse.

```
clickhouse/
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ 001_create_iceberg_tables.sql  # Create tables vá»›i IcebergS3 engine
â””â”€â”€ queries/
    â””â”€â”€ sample_queries.sql              # Sample queries
```

**Chá»©c nÄƒng:**
- Táº¡o tables trong ClickHouse
- Sample queries Ä‘á»ƒ test
- Migrations Ä‘á»ƒ version control schema

**CÃ¡ch cháº¡y:**
```bash
make clickhouse-migrate    # Cháº¡y migrations
make clickhouse-client     # Má»Ÿ ClickHouse client
```

---

### 5. `superset/` - Apache Superset

Chá»©a dashboards vÃ  setup script cho Superset.

```
superset/
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ dashboard_config.json  # Dashboard configuration
â””â”€â”€ setup_superset.py          # Auto-setup script
```

**Chá»©c nÄƒng:**
- Setup Superset connection Ä‘áº¿n ClickHouse
- Import dashboard templates
- Táº¡o charts vÃ  dashboards tá»± Ä‘á»™ng

**CÃ¡ch cháº¡y:**
```bash
make setup-superset    # Cháº¡y setup script
```

---

### 6. `data/` - Raw Data

Chá»©a dá»¯ liá»‡u thÃ´ (CSV files) tá»« Kaggle dataset.

```
data/
â””â”€â”€ raw/
    â”œâ”€â”€ 2019-Oct.csv
    â”œâ”€â”€ 2019-Nov.csv
    â”œâ”€â”€ 2019-Dec.csv
    â”œâ”€â”€ 2020-Jan.csv
    â””â”€â”€ 2020-Feb.csv
```

**Chá»©c nÄƒng:**
- LÆ°u trá»¯ raw data files
- Input cho Bronze layer ingestion

**LÆ°u Ã½:**
- Files nÃ y cáº§n Ä‘Æ°á»£c download tá»« Kaggle
- Äáº·t vÃ o `data/raw/` trÆ°á»›c khi cháº¡y pipeline

---

### 7. `scripts/` - Utility Scripts

Chá»©a cÃ¡c scripts tiá»‡n Ã­ch.

```
scripts/
â”œâ”€â”€ setup.sh           # Initial setup script
â”œâ”€â”€ run_pipeline.sh    # Run full pipeline
â””â”€â”€ generate_sample_data.py  # Generate sample data (optional)
```

**Chá»©c nÄƒng:**
- `setup.sh`: Kiá»ƒm tra mÃ´i trÆ°á»ng, táº¡o thÆ° má»¥c, build images
- `run_pipeline.sh`: Cháº¡y toÃ n bá»™ pipeline tá»« Bronze â†’ ClickHouse
- `generate_sample_data.py`: Táº¡o dá»¯ liá»‡u máº«u (náº¿u cáº§n)

**CÃ¡ch cháº¡y:**
```bash
./scripts/setup.sh
./scripts/run_pipeline.sh
```

---

### 8. `docs/` - Documentation

Chá»©a tÃ i liá»‡u ká»¹ thuáº­t.

```
docs/
â””â”€â”€ ARCHITECTURE.md    # Kiáº¿n trÃºc há»‡ thá»‘ng chi tiáº¿t
```

**Chá»©c nÄƒng:**
- Giáº£i thÃ­ch kiáº¿n trÃºc há»‡ thá»‘ng
- Data flow diagrams
- Technology stack details

---

## ğŸ”‘ CÃ¡c File Quan Trá»ng

### `Makefile`

File chá»©a táº¥t cáº£ automation commands.

**CÃ¡c nhÃ³m lá»‡nh:**
- **Infrastructure**: `make up`, `make down`, `make logs`
- **Pipeline**: `make pipeline-full`, `make ingest-bronze`, etc.
- **Development**: `make spark-shell`, `make clickhouse-client`
- **dbt**: `make dbt-run`, `make dbt-test`

**Xem táº¥t cáº£ lá»‡nh:**
```bash
make help
```

---

### `README.md`

File tá»•ng quan vá» project.

**Ná»™i dung:**
- Giá»›i thiá»‡u project
- Kiáº¿n trÃºc há»‡ thá»‘ng
- Quick start
- Technology stack
- Links Ä‘áº¿n cÃ¡c tÃ i liá»‡u khÃ¡c

---

### `HUONG_DAN_CHAY_PROJECT.md`

TÃ i liá»‡u hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ báº±ng tiáº¿ng Viá»‡t.

**Ná»™i dung:**
- HÆ°á»›ng dáº«n setup tá»«ng bÆ°á»›c
- Cháº¡y pipeline
- Troubleshooting
- CÃ¡c lá»‡nh thÆ°á»ng dÃ¹ng

---

### `QUICK_START.md`

HÆ°á»›ng dáº«n nhanh Ä‘á»ƒ báº¯t Ä‘áº§u.

**Ná»™i dung:**
- 3 bÆ°á»›c Ä‘Æ¡n giáº£n
- Links Ä‘áº¿n tÃ i liá»‡u Ä‘áº§y Ä‘á»§

---

## ğŸ”„ Data Flow

```
1. data/raw/*.csv
   â†“
2. spark/jobs/bronze/ingest_events.py
   â†“
3. Iceberg Bronze Tables (MinIO)
   â†“
4. spark/jobs/silver/clean_events.py
   â†“
5. Iceberg Silver Tables (MinIO)
   â†“
6. spark/jobs/gold/aggregate_sales.py
   â†“
7. Iceberg Gold Tables (MinIO)
   â†“
8. spark/jobs/serving/sync_to_clickhouse.py
   â†“
9. ClickHouse Tables
   â†“
10. Superset Dashboards
```

---

## ğŸ› ï¸ Development Workflow

### 1. ThÃªm Spark Job Má»›i

```bash
# Táº¡o file má»›i trong spark/jobs/
vim spark/jobs/bronze/my_new_job.py

# Test job
make spark-pyspark
# Hoáº·c
docker exec spark-master spark-submit \
    --master spark://spark-master:7077 \
    /opt/spark-apps/jobs/bronze/my_new_job.py
```

### 2. ThÃªm dbt Model Má»›i

```bash
# Táº¡o file SQL trong dbt/models/
vim dbt/models/marts/core/my_new_model.sql

# Test model
cd dbt
dbt run --select my_new_model
dbt test --select my_new_model
```

### 3. ThÃªm ClickHouse Table Má»›i

```bash
# Táº¡o migration file
vim clickhouse/migrations/002_create_new_table.sql

# Cháº¡y migration
make clickhouse-migrate
```

### 4. ThÃªm Superset Dashboard

```bash
# Táº¡o dashboard config
vim superset/dashboards/my_dashboard.json

# Import trong Superset UI hoáº·c qua API
python superset/setup_superset.py
```

---

## ğŸ“Š CÃ¡c Báº£ng Dá»¯ Liá»‡u

### Bronze Layer

| Table | Description | Location |
|-------|-------------|----------|
| `iceberg.bronze.events_raw` | Raw events tá»« CSV | MinIO: `s3://iceberg-warehouse/bronze/` |

### Silver Layer

| Table | Description | Location |
|-------|-------------|----------|
| `iceberg.silver.events_cleaned` | Events Ä‘Ã£ lÃ m sáº¡ch | MinIO: `s3://iceberg-warehouse/silver/` |
| `iceberg.silver.dim_products` | Product dimension | MinIO: `s3://iceberg-warehouse/silver/` |
| `iceberg.silver.dim_users` | User dimension | MinIO: `s3://iceberg-warehouse/silver/` |

### Gold Layer

| Table | Description | Location |
|-------|-------------|----------|
| `iceberg.gold.daily_sales_by_category` | Daily sales | MinIO: `s3://iceberg-warehouse/gold/` |
| `iceberg.gold.funnel_analysis` | Conversion funnel | MinIO: `s3://iceberg-warehouse/gold/` |
| `iceberg.gold.customer_rfm` | RFM segmentation | MinIO: `s3://iceberg-warehouse/gold/` |
| `iceberg.gold.product_performance` | Product metrics | MinIO: `s3://iceberg-warehouse/gold/` |

### ClickHouse Tables

| Table | Description | Engine |
|-------|-------------|--------|
| `lakehouse.daily_sales` | Daily sales | IcebergS3 |
| `lakehouse.funnel_analysis` | Funnel metrics | IcebergS3 |
| `lakehouse.customer_rfm` | Customer segments | IcebergS3 |
| `lakehouse.product_performance` | Product metrics | IcebergS3 |

---

## ğŸ” TÃ¬m Kiáº¿m File

### TÃ¬m Spark Jobs

```bash
find spark/jobs -name "*.py"
```

### TÃ¬m dbt Models

```bash
find dbt/models -name "*.sql"
```

### TÃ¬m Configuration Files

```bash
find . -name "*.yml" -o -name "*.yaml"
find . -name "*.conf" -o -name "*.xml"
```

---

## ğŸ“ Best Practices

1. **Naming Convention:**
   - Spark jobs: `snake_case.py`
   - dbt models: `snake_case.sql`
   - Tables: `snake_case` hoáº·c `camelCase`

2. **File Organization:**
   - Má»—i layer (bronze/silver/gold) trong thÆ° má»¥c riÃªng
   - Related files nÃªn á»Ÿ gáº§n nhau

3. **Documentation:**
   - Comment code rÃµ rÃ ng
   - Update README khi thÃªm feature má»›i
   - Document schema changes

4. **Version Control:**
   - Commit thÆ°á»ng xuyÃªn
   - Use meaningful commit messages
   - Tag releases

---

**Happy Coding! ğŸš€**
